{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfyv172-s2YH"
      },
      "outputs": [],
      "source": [
        "# # Running this code will query a table in BigQuery and download\n",
        "# # the results to a Pandas DataFrame named `results`.\n",
        "# # Learn more here: https://cloud.google.com/bigquery/docs/visualize-jupyter\n",
        "\n",
        "# %%bigquery results --project direct-plateau-431009-i2\n",
        "# SELECT * FROM `direct-plateau-431009-i2.123.lstm_all_icu_labevents` where valuenum is not null  #this example uses a penguin public dataset. Learn more here: https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=ml_datasets&t=penguins&page=table&_ga=2.251359750.1031997792.1692116300-1119797950.1692116300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIOEpVHFw3y1"
      },
      "source": [
        "# Connect google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYNYCtIOtlZ6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_m09vX-_S7E",
        "outputId": "99197f8c-2dc1-44a5-846d-9f0809b727ab"
      },
      "outputs": [],
      "source": [
        "# !pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVvu7QnF1fWC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, Flatten, Activation, RepeatVector, Permute, Multiply, Attention\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEUw25OEw9Ok"
      },
      "source": [
        "# Import bigquery package, and query the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VJ5rvP0tqE7"
      },
      "outputs": [],
      "source": [
        "# from google.cloud import bigquery\n",
        "\n",
        "# project_id = 'direct-plateau-431009-i2'\n",
        "# client = bigquery.Client(project=project_id)\n",
        "\n",
        "# query = \"\"\"\n",
        "# SELECT *\n",
        "# FROM `direct-plateau-431009-i2.123.lstm_all_icu_labevents`\n",
        "\n",
        "# \"\"\"\n",
        "# # WHERE valuenum IS NOT NULL\n",
        "# query_job = client.query(query)\n",
        "\n",
        "# # Convert the query result to a Pandas DataFrame\n",
        "# df = query_job.to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0fN1szeWs3Xt",
        "outputId": "a1de6be2-893e-49f9-c2fa-8a427b2644b9"
      },
      "outputs": [],
      "source": [
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiUmJhsaxcO0"
      },
      "source": [
        "# Filter the selected_labels from df about AKI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab1vG-UZw16K"
      },
      "outputs": [],
      "source": [
        "selected_labels = [\n",
        "   'Blood Urea Nitrogen','Creatinine','Glucose','Potassium','Calcium','Platelet Count','White Blood Cell Count',\n",
        "   'Bicarbonate','Oxygen Saturation']\n",
        "\n",
        "# filter the selected_labels from df\n",
        "df_filtered = df[df['label'].isin(selected_labels)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xExdR1XNuPlI",
        "outputId": "2dc97d4b-8893-4392-f8cb-7c047181b114"
      },
      "outputs": [],
      "source": [
        "df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duYISuGrxmHb"
      },
      "source": [
        "# According survival_days to compute the categorize of survival_days, named categorize_survival_days, and add into df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LPJU8AVt7c0",
        "outputId": "1df5083c-0a98-4678-b323-c6972f70ce1e"
      },
      "outputs": [],
      "source": [
        "def categorize_survival_days(days):\n",
        "    if days <= 200:\n",
        "        return '0'  #Short-term survival\n",
        "    elif 200 < days <= 800:\n",
        "        return '1'  #Medium-term survival\n",
        "    elif 800 < days <= 1500:\n",
        "        return '2'  #Long-term survival\n",
        "    else:\n",
        "        return '3'  #Very long-term survival\n",
        "\n",
        "# the category of survival_days\n",
        "df_filtered['survival_category'] = df_filtered['survival_days'].apply(categorize_survival_days)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7WhWqoVcuTZa",
        "outputId": "abb1d219-3e37-4bed-a617-820638e58270"
      },
      "outputs": [],
      "source": [
        "df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX0wElvHyOEA"
      },
      "source": [
        "# use pivot_table to convert data to wide table form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N982htsBu8Z5",
        "outputId": "76d36daa-2468-42d9-8aa0-e72197b79539"
      },
      "outputs": [],
      "source": [
        "# use pivot_table to convert data to wide table form\n",
        "df_pivot = df_filtered.pivot_table(index=['subject_id', 'charttime', 'gender', 'aki_age', 'bmi', 'icu_num', 'survival_category'],\n",
        "                          columns='label', values='valuenum').reset_index()\n",
        "\n",
        "df_pivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUhSA96cyUAM"
      },
      "source": [
        "# compute the missing value in df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F5BZ8_kyUO3",
        "outputId": "2d6b67fd-68d8-4e73-88d9-b440ef125441"
      },
      "outputs": [],
      "source": [
        "print(df_pivot.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txce_MIhybCE"
      },
      "source": [
        "# Missing value processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Zx-5vQdUvIaD",
        "outputId": "64142320-69f6-4006-8e1a-747dfcef37ab"
      },
      "outputs": [],
      "source": [
        " # Remove all columns with missing values\n",
        "df_cleaned = df_pivot.dropna(axis=1, how='all')\n",
        "\n",
        "# print the shape of the cleaned DataFrame\n",
        "print(\"Shape after removal of all empty columns:\", df_cleaned.shape)\n",
        "\n",
        "# Setting the missing value threshold\n",
        "threshold = 0.5  #\n",
        "\n",
        "# Remove rows with missing values above a threshold value\n",
        "df_cleaned_new = df_cleaned.dropna(thresh=int(threshold * df_cleaned.shape[1]))\n",
        "\n",
        "df_cleaned_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_QLV_xxyuEw",
        "outputId": "50d09c85-4f17-47e3-ff5a-3b0aff972a64"
      },
      "outputs": [],
      "source": [
        "# Check if there are still missing values in the df_cleaned_new DataFrame\n",
        "df_cleaned_new_missing_value = df_cleaned_new.isnull().sum()\n",
        "print(\"Missing value statistics after missing value processing:\")\n",
        "print(df_cleaned_new_missing_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqWEaHoSzQvj"
      },
      "source": [
        "# To confirm the LSTM model of time_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddQ-6X7pvLTG",
        "outputId": "a19708d2-608b-4898-a847-7d4ac03ee800"
      },
      "outputs": [],
      "source": [
        "# Check the amount of data per patient\n",
        "patient_data_count = df_cleaned_new.groupby('subject_id').size()\n",
        "\n",
        "# print the count of per patient\n",
        "print(patient_data_count.describe())\n",
        "\n",
        "# Print the number of patients with data less than time_steps\n",
        "min_time_steps = 5  # set time_steps= 5\n",
        "insufficient_data_patients = (patient_data_count < min_time_steps).sum()\n",
        "print(f\"Number of patients with fewer data than {min_time_steps}: {insufficient_data_patients}\")\n",
        "\n",
        "# Choosing the right time step\n",
        "time_steps = min(patient_data_count.median(), 30)  #\n",
        "print(f\"Choosing the time step: {time_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5GbBi2C0_sN"
      },
      "source": [
        "# Missing value processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxnWacV91Cjx"
      },
      "outputs": [],
      "source": [
        "# Sort by subject_id and charttime\n",
        "df_cleaned_new.sort_values(by=['subject_id', 'charttime'], inplace=True)\n",
        "\n",
        "# Using MICE to Fill in Missing Values in Value Columns\n",
        "numeric_columns = df_cleaned_new.select_dtypes(include=[float, int]).columns\n",
        "mice_imputer = IterativeImputer(max_iter=10, random_state=42, imputation_order='ascending', initial_strategy='mean')\n",
        "df_numeric_imputed = pd.DataFrame(mice_imputer.fit_transform(df_cleaned_new[numeric_columns]), columns=numeric_columns)\n",
        "\n",
        "# Combine interpolated numeric columns with other columns in the original dataset\n",
        "df_imputed = df_cleaned_new.copy()\n",
        "df_imputed[numeric_columns] = df_numeric_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KZtaAYBH1abh",
        "outputId": "fa9651ec-245c-4eb8-ff01-9f1a0dbe517e"
      },
      "outputs": [],
      "source": [
        "df_imputed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDkxj-kl023F"
      },
      "source": [
        "# Selection of features to be normalised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "x9CNOo9V2Ol_",
        "outputId": "fda5df1c-12c2-4694-f120-f0091c74ba30"
      },
      "outputs": [],
      "source": [
        "columns_to_normalize = [\n",
        "    'bmi', 'aki_age','Bicarbonate', 'Creatinine', 'Glucose', 'Oxygen Saturation', 'Platelet Count', 'Potassium'\n",
        "    ]\n",
        "\n",
        "# Initialise the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalise the specified columns and store the result in a new DataFrame\n",
        "df_final = df_imputed.copy()\n",
        "df_final[columns_to_normalize] = scaler.fit_transform(df_imputed[columns_to_normalize])\n",
        "\n",
        "df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tDXOKg32AGG"
      },
      "source": [
        "# Compute the count of survival_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuuQRn_F02AD",
        "outputId": "44fbc8fd-ee55-42e5-ca91-8d9a8aa703fc"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of each category in survival_category\n",
        "category_counts = df_final['survival_category'].value_counts()\n",
        "\n",
        "# Calculation ratio\n",
        "total_samples = len(df_final)\n",
        "category_proportions = category_counts / total_samples\n",
        "\n",
        "print(category_counts)\n",
        "print(category_proportions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTqxQjvd3ASu"
      },
      "source": [
        "# Check for missing values again to confirm there are no missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBAMOIY93VtD",
        "outputId": "756a19fd-2792-425e-a40e-14818bdec4b5"
      },
      "outputs": [],
      "source": [
        "# Check if there are still missing values in the df_cleaned_new DataFrame\n",
        "df_final_missing_value = df_final.isnull().sum()\n",
        "print(\"Missing value statistics for df_final:\")\n",
        "print(df_final_missing_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkOEL7YE3iLi"
      },
      "source": [
        "# Standardisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWVkTZv23mz4",
        "outputId": "01ea2b99-be80-41f4-dd66-c773297a39a9"
      },
      "outputs": [],
      "source": [
        "# choose the feature\n",
        "features_to_scale = ['Bicarbonate', 'Creatinine', 'Glucose', 'Oxygen Saturation', 'Platelet Count', 'Potassium']\n",
        "\n",
        "# Standard the data\n",
        "scaler = StandardScaler()\n",
        "df_final[features_to_scale] = scaler.fit_transform(df_final[features_to_scale])\n",
        "\n",
        "print(f\"Completion of standardisation, dataframe named df_final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2DySteF4GNo"
      },
      "source": [
        "# Processing data into structures usable for LSTM model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUm7l0Qv4KuX",
        "outputId": "ae1800ce-71ab-4836-96cf-79bf131e52ad"
      },
      "outputs": [],
      "source": [
        "# Constructing LSTM time-series data\n",
        "def create_sequences(data, time_steps=22):\n",
        "    X, X_static, y = [], [], []\n",
        "    for _, group in data.groupby('subject_id'):\n",
        "        feature_data = group[features_to_scale].values\n",
        "        static_data = group[['gender', 'aki_age', 'bmi']].values\n",
        "        target_data = group['survival_category'].values\n",
        "\n",
        "        if len(feature_data) >= time_steps:\n",
        "            for i in range(len(feature_data) - time_steps + 1):\n",
        "                X.append(feature_data[i:i+time_steps])\n",
        "                X_static.append(static_data[i+time_steps-1])  # 取最后一个时间步的静态特征\n",
        "                y.append(target_data[i+time_steps-1])\n",
        "    return np.array(X), np.array(X_static), np.array(y)\n",
        "\n",
        "# Constructing data\n",
        "time_steps = time_steps\n",
        "X_sequence, X_static, y = create_sequences(df_final, time_steps)\n",
        "\n",
        "# Convert target tag \"y\"  to one-hot code\n",
        "y_categorical = to_categorical(y, num_classes=4)\n",
        "\n",
        "# split dataset test_size20% and random_state set 42\n",
        "X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(\n",
        "    X_sequence, X_static, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# To ensure that X_train and y_train are floating-point types\n",
        "X_seq_train = np.array(X_seq_train, dtype=np.float32)\n",
        "X_seq_test = np.array(X_seq_test, dtype=np.float32)\n",
        "X_static_train = np.array(X_static_train, dtype=np.float32)\n",
        "X_static_test = np.array(X_static_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "print(\"success\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3IElnJA6JS_"
      },
      "source": [
        "# Check the data shape and type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b2q-3no4rkF",
        "outputId": "1ade22ad-9818-4460-9782-125b38e4e52e"
      },
      "outputs": [],
      "source": [
        "# check the shape and type\n",
        "print(f\"X_seq_train shape: {X_seq_train.shape}, dtype: {X_seq_train.dtype}\")\n",
        "print(f\"X_static_train shape: {X_static_train.shape}, dtype: {X_static_train.dtype}\")\n",
        "print(f\"y_train shape: {y_train.shape}, dtype: {y_train.dtype}\\n\")\n",
        "\n",
        "print(f\"X_seq_test shape: {X_seq_test.shape}, dtype: {X_seq_test.dtype}\")\n",
        "print(f\"X_static_test shape: {X_static_test.shape}, dtype: {X_static_test.dtype}\")\n",
        "print(f\"y_test shape: {y_test.shape}, dtype: {y_test.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htbDFavmR2-4"
      },
      "source": [
        "# Processing data imbalances with smote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsMkoqCa9BIl",
        "outputId": "cd7c7d92-ab37-47e1-f394-fe0b5ac5a6b9"
      },
      "outputs": [],
      "source": [
        "# Reshape for SMOTE\n",
        "X_seq_train_reshaped = X_seq_train.reshape(X_seq_train.shape[0], -1)\n",
        "\n",
        "# combine data\n",
        "X_static_train_reshaped = X_static_train.reshape(X_static_train.shape[0], -1)\n",
        "\n",
        "# Combine sequence and static features\n",
        "combined_X_train = np.hstack([X_seq_train_reshaped, X_static_train_reshaped])\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "combined_X_train_resampled, y_train_resampled = smote.fit_resample(combined_X_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "# Split the resampled data back into sequence and static features\n",
        "X_seq_train_resampled = combined_X_train_resampled[:, :X_seq_train_reshaped.shape[1]]\n",
        "X_static_train_resampled = combined_X_train_resampled[:, X_seq_train_reshaped.shape[1]:]\n",
        "\n",
        "# Reshape sequence data back to its original shape\n",
        "X_seq_train_resampled = X_seq_train_resampled.reshape(-1, time_steps, len(features_to_scale))\n",
        "y_train_resampled = to_categorical(y_train_resampled, num_classes=4)\n",
        "\n",
        "# check the shape\n",
        "print(X_seq_train_resampled.shape)\n",
        "print(X_static_train_resampled.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0EdUc4F9bjq"
      },
      "source": [
        "# check the category count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8LehTzG9d9e",
        "outputId": "83dd7e06-263f-4670-ae8d-b1dfea0e4ac8"
      },
      "outputs": [],
      "source": [
        "y_train_labels = np.argmax(y_train_resampled, axis=1)\n",
        "\n",
        "# Calculate the number of samples per category using np.bincount\n",
        "class_distribution = np.bincount(y_train_labels)\n",
        "\n",
        "for label, count in zip(np.unique(y_train_labels), class_distribution):\n",
        "    print(f\"Class {label}: {count} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2SSu2gw7fFs"
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "def build_lstm_model():\n",
        "    sequence_input = Input(shape=(X_seq_train.shape[1], X_seq_train.shape[2]), name='sequence_input')\n",
        "    static_input = Input(shape=(X_static_train.shape[1],), name='static_input')\n",
        "\n",
        "    # LSTM layer\n",
        "    x = LSTM(64, return_sequences=True)(sequence_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = LSTM(32)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Connecting static features to LSTM outputs\n",
        "    x = Concatenate()([x, static_input])\n",
        "\n",
        "    # full layer\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    # set model\n",
        "    model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "#\n",
        "# model = build_lstm_model()\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbSR-lET78XM"
      },
      "source": [
        "# set EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSh4NW9Q7fH_"
      },
      "outputs": [],
      "source": [
        "# creat EarlyStopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # val loss\n",
        "    patience=5,          #\n",
        "    verbose=1,           # log detail\n",
        "    mode='min',          # val loss min\n",
        "    restore_best_weights=True  #\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX1sMfAz_nyo"
      },
      "source": [
        "# Fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU68pQ_g_BMj",
        "outputId": "116c3591-19d7-4616-c006-002d04da08cb"
      },
      "outputs": [],
      "source": [
        "# split validation dataset\n",
        "X_seq_train_final, X_seq_val, X_static_train_final, X_static_val, y_train_final, y_val = train_test_split(\n",
        "    X_seq_train_resampled, X_static_train_resampled, y_train_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# check the shape of dataset\n",
        "print(f\"Training Sequence Data Shape: {X_seq_train_final.shape}\")\n",
        "print(f\"Validation Sequence Data Shape: {X_seq_val.shape}\")\n",
        "print(f\"Training Static Data Shape: {X_static_train_final.shape}\")\n",
        "print(f\"Validation Static Data Shape: {X_static_val.shape}\")\n",
        "print(f\"Training Labels Shape: {y_train_final.shape}\")\n",
        "print(f\"Validation Labels Shape: {y_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gIYWYWvp7G3"
      },
      "outputs": [],
      "source": [
        "# # set model\n",
        "# model = build_lstm_model()\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRQb2LWPi0Q-",
        "outputId": "280df8a0-4280-4a55-832f-4a30dcad006b"
      },
      "outputs": [],
      "source": [
        "# set model\n",
        "model = build_lstm_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "history = model.fit([X_seq_train_final, X_static_train_final], y_train_final,\n",
        "                    validation_data=([X_seq_val, X_static_val], y_val),\n",
        "                    epochs=100,\n",
        "                    batch_size=64,\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5inC9QfGHOyI",
        "outputId": "126affd6-1a51-4d4c-9680-3453c31ba4b1"
      },
      "outputs": [],
      "source": [
        "# Plot loss function trends for training and validation\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM model - Loss Function Trend')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQJNOmn8DGml"
      },
      "source": [
        "# Evaluate the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xiWrY9-_X7t",
        "outputId": "76cc1ee4-d640-468d-9bd0-af01d8b965f0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "test_loss, test_acc = model.evaluate([X_seq_test, X_static_test], y_test, verbose=2)\n",
        "print(f\"Initial test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "y_pred = model.predict([X_seq_test, X_static_test], verbose=2)\n",
        "\n",
        "# one hot processing\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_true_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# calculate precision recall and F1-score\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# print Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# print Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "Q3VSv9s66gVO",
        "outputId": "fb3edc45-ca94-46ca-f1f1-74785ae0ebb9"
      },
      "outputs": [],
      "source": [
        "# For testing\n",
        "\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "# n_classes = 4\n",
        "\n",
        "\n",
        "# y_test_binarized = label_binarize(y_true_classes, classes=[0, 1, 2, 3])\n",
        "# y_pred_probabilities = y_pred  # 假设 y_pred 是模型输出的概率\n",
        "\n",
        "\n",
        "# # weighted_auc_score = roc_auc_score(y_test_binarized, y_pred_probabilities, average='weighted', multi_class='ovr')\n",
        "# # print(f\"LSTM Model Weighted AUC: {weighted_auc_score:.4f}\")\n",
        "\n",
        "\n",
        "# fpr_weighted, tpr_weighted, _ = roc_curve(y_test_binarized.ravel(), y_pred_probabilities.ravel())\n",
        "\n",
        "\n",
        "# auc_score = roc_auc_score(y_test_binarized, y_pred_probabilities, average='weighted', multi_class='ovr')\n",
        "# print(f\"Weighted AUC: {auc_score:.4f}\")\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# for i in range(n_classes):\n",
        "#     fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])\n",
        "#     plt.plot(fpr, tpr, label=f'Class {i} ROC curve (area = {auc(fpr, tpr):.4f})')\n",
        "\n",
        "\n",
        "# plt.plot(fpr_weighted, tpr_weighted, label=f'Weighted ROC curve (area = {auc_score:.4f})', color='red', linestyle='--', linewidth=2)\n",
        "\n",
        "# plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "# plt.xlim([0.0, 1.0])\n",
        "# plt.ylim([0.0, 1.05])\n",
        "# plt.xlabel('False Positive Rate')\n",
        "# plt.ylabel('True Positive Rate')\n",
        "# plt.title('LSTM Model ROC Curves')\n",
        "# plt.legend(loc=\"lower right\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "nw-ebCGq9a4b",
        "outputId": "bf19e57c-88bd-43e1-900c-7f8d9d89cb1c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from PIL import Image\n",
        "\n",
        "# 假设 y_true_classes 是 LSTM 模型的真实类标签，y_pred 是模型的预测概率\n",
        "n_classes = 4\n",
        "\n",
        "# 如果 y_true_classes 还没有二值化\n",
        "y_test_binarized = label_binarize(y_true_classes, classes=[0, 1, 2, 3])\n",
        "y_pred_probabilities = y_pred  # 假设 y_pred 是模型输出的概率\n",
        "\n",
        "# 计算加权 AUC\n",
        "weighted_auc_score = roc_auc_score(y_test_binarized, y_pred_probabilities, average='weighted', multi_class='ovr')\n",
        "print(f\"LSTM Model Weighted AUC: {weighted_auc_score:.4f}\")\n",
        "\n",
        "# 计算加权 ROC 曲线\n",
        "fpr_weighted, tpr_weighted, _ = roc_curve(y_test_binarized.ravel(), y_pred_probabilities.ravel())\n",
        "\n",
        "# 1. 单独绘制加权 ROC 曲线\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr_weighted, tpr_weighted, label=f'Weighted ROC curve (area = {weighted_auc_score:.4f})', color='red', linestyle='--', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('LSTM Model - Weighted ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('weighted_roc_curve.png')  # 保存加权 ROC 曲线图像\n",
        "plt.show()\n",
        "\n",
        "# 2. 单独绘制每个类别的 ROC 曲线\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} ROC curve (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('LSTM Model - Class-wise ROC Curves')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('classwise_roc_curve.png')  # 保存分类 ROC 曲线图像\n",
        "plt.show()\n",
        "\n",
        "# 3. 将两张图拼接成一张左右展示的大图\n",
        "# 加载图像\n",
        "img1 = Image.open('weighted_roc_curve.png')\n",
        "img2 = Image.open('classwise_roc_curve.png')\n",
        "\n",
        "# 确保图像宽度相对较大，图像高度相对较小\n",
        "img1 = img1.resize((int(img1.width * 1.5), int(img1.height * 0.75)))\n",
        "img2 = img2.resize((int(img2.width * 1.5), int(img2.height * 0.75)))\n",
        "\n",
        "# 创建新的图像来水平拼接\n",
        "new_img = Image.new('RGB', (img1.width + img2.width, max(img1.height, img2.height)))\n",
        "new_img.paste(img1, (0, 0))\n",
        "new_img.paste(img2, (img1.width, 0))\n",
        "\n",
        "# 显示拼接后的图像\n",
        "new_img.show()\n",
        "\n",
        "# 保存拼接后的图像\n",
        "new_img.save('combined_roc_curves_side_by_side.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "7a2xuO23_KHh",
        "outputId": "bb40509b-8999-472f-9363-a9da6814f468"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 假设 y_true_classes 是 LSTM 模型的真实类标签，y_pred 是模型的预测概率\n",
        "n_classes = 4\n",
        "\n",
        "# 如果 y_test 还没有二值化\n",
        "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
        "y_pred_probabilities = model.predict([X_seq_test, X_static_test])  # 使用训练好的 LSTM 模型进行预测\n",
        "\n",
        "# 计算每个类别的 AUC\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(n_classes):\n",
        "    auc_score = roc_auc_score(y_test_binarized[:, i], y_pred_probabilities[:, i])\n",
        "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} AUC = {auc_score:.4f}')\n",
        "\n",
        "# 绘制微平均 ROC 曲线\n",
        "fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), y_pred_probabilities.ravel())\n",
        "roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "plt.plot(fpr_micro, tpr_micro, linestyle='--', color='black', lw=2, label=f'Micro-Average AUC = {roc_auc_micro:.4f}')\n",
        "\n",
        "# 图形美化\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # 对角线\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('LSTM Model - ROC Curves with AUC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPb3tkTy28-l"
      },
      "source": [
        "# AUC 评估的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "B2iERUv127sW",
        "outputId": "fd2c7957-0c00-48bb-ff70-8c4b4c1211b2"
      },
      "outputs": [],
      "source": [
        "# # import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "# from sklearn.preprocessing import label_binarize\n",
        "# from PIL import Image\n",
        "\n",
        "# # 假设 y_true_classes 是 LSTM 模型的真实类标签，y_pred 是模型的预测概率\n",
        "# n_classes = 4\n",
        "\n",
        "# # 如果 y_test 还没有二值化\n",
        "# y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
        "# y_pred_probabilities = model.predict([X_seq_test, X_static_test])  # 使用训练好的 LSTM 模型进行预测\n",
        "\n",
        "# # 1. 计算并绘制微平均和宏平均 AUC 曲线\n",
        "# plt.figure(figsize=(6, 4))\n",
        "\n",
        "# # 微平均 AUC\n",
        "# fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), y_pred_probabilities.ravel())\n",
        "# roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "# plt.plot(fpr_micro, tpr_micro, linestyle='--', color='black', lw=2, label=f'Micro-Average AUC = {roc_auc_micro:.4f}')\n",
        "\n",
        "# # 宏平均 AUC\n",
        "# all_fpr = np.unique(np.concatenate([roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])[0] for i in range(n_classes)]))\n",
        "# mean_tpr = np.zeros_like(all_fpr)\n",
        "# for i in range(n_classes):\n",
        "#     mean_tpr += np.interp(all_fpr, roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])[0],\n",
        "#                           roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])[1])\n",
        "\n",
        "# mean_tpr /= n_classes\n",
        "# roc_auc_macro = auc(all_fpr, mean_tpr)\n",
        "# plt.plot(all_fpr, mean_tpr, linestyle='-', color='blue', lw=2, label=f'Macro-Average AUC = {roc_auc_macro:.4f}')\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(10, 8))\n",
        "\n",
        "# # 1. 微平均 ROC 曲线和 AUC\n",
        "# fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), y_pred_probabilities.ravel())\n",
        "# roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "# plt.plot(fpr_micro, tpr_micro, linestyle='--', color='black', lw=2, label=f'Micro-Average AUC = {roc_auc_micro:.4f}')\n",
        "\n",
        "# # 2. 宏平均 ROC 曲线和 AUC\n",
        "# all_fpr = np.unique(np.concatenate([roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])[0] for i in range(n_classes)]))\n",
        "# mean_tpr = np.zeros_like(all_fpr)\n",
        "# for i in range(n_classes):\n",
        "#     mean_tpr += np.interp(all_fpr, roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])[0],\n",
        "#                           roc_curve(y_test_binarized[:, i], y_pred_probabilities[:, i])[1])\n",
        "\n",
        "# mean_tpr /= n_classes\n",
        "# roc_auc_macro = auc(all_fpr, mean_tpr)\n",
        "# plt.plot(all_fpr, mean_tpr, linestyle='-', color='blue', lw=2, label=f'Macro-Average AUC = {roc_auc_macro:.4f}')\n",
        "\n",
        "# # 3. 图形美化和展示\n",
        "# plt.plot([0, 1], [0, 1], 'k--', lw=2)  # 对角线\n",
        "# plt.xlim([0.0, 1.0])\n",
        "# plt.ylim([0, 1.05])\n",
        "# plt.grid(False)\n",
        "# plt.xlabel('False Positive Rate')\n",
        "# plt.ylabel('True Positive Rate')\n",
        "# plt.title('LSTM Model - Micro & Macro Average AUC')\n",
        "# plt.legend(loc=\"lower right\")\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtmVDqn8x8Z5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 假设你有以下数据\n",
        "# X_seq_test, X_static_test, y_test\n",
        "\n",
        "# 将y_test转换为类标签\n",
        "y_true_classes = y_test.argmax(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80RAapOJx8el",
        "outputId": "862ecd58-908b-472f-f0b5-03621916ed8d"
      },
      "outputs": [],
      "source": [
        "def calculate_auc_scores(model, X_seq_test, X_static_test, y_test, cv=10):\n",
        "    skf = StratifiedKFold(n_splits=cv)\n",
        "    micro_auc_scores = []\n",
        "    weighted_auc_scores = []\n",
        "\n",
        "    y_true = y_test.argmax(axis=1)\n",
        "\n",
        "    for train_index, test_index in skf.split(X_seq_test, y_true):\n",
        "        X_seq_train, X_seq_val = X_seq_test[train_index], X_seq_test[test_index]\n",
        "        X_static_train, X_static_val = X_static_test[train_index], X_static_test[test_index]\n",
        "        y_train, y_val = y_test[train_index], y_test[test_index]\n",
        "\n",
        "        model.fit([X_seq_train, X_static_train], y_train, epochs=10, batch_size=32, verbose=0)\n",
        "        y_pred = model.predict([X_seq_val, X_static_val])\n",
        "\n",
        "        # 计算Micro-average AUC\n",
        "        micro_auc = roc_auc_score(y_val, y_pred, average=\"micro\", multi_class=\"ovr\")\n",
        "        micro_auc_scores.append(micro_auc)\n",
        "\n",
        "        # 计算Weighted-average AUC\n",
        "        weighted_auc = roc_auc_score(y_val, y_pred, average=\"weighted\", multi_class=\"ovr\")\n",
        "        weighted_auc_scores.append(weighted_auc)\n",
        "\n",
        "    return micro_auc_scores, weighted_auc_scores\n",
        "\n",
        "micro_auc_scores, weighted_auc_scores = calculate_auc_scores(model, X_seq_test, X_static_test, y_test)\n",
        "\n",
        "print(f\"Micro-average AUC: {np.mean(micro_auc_scores):.4f} ± {np.std(micro_auc_scores):.4f}\")\n",
        "print(f\"Weighted-average AUC: {np.mean(weighted_auc_scores):.4f} ± {np.std(weighted_auc_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uZJBEL2-uQN"
      },
      "outputs": [],
      "source": [
        "def calculate_auc_scores(model, X_seq_test, X_static_test, y_test, cv=10):\n",
        "    skf = StratifiedKFold(n_splits=cv)\n",
        "    micro_auc_scores = []\n",
        "    weighted_auc_scores = []\n",
        "\n",
        "    y_true = y_test.argmax(axis=1)\n",
        "    last_y_val = None\n",
        "    last_y_pred = None\n",
        "\n",
        "    for train_index, test_index in skf.split(X_seq_test, y_true):\n",
        "        X_seq_train, X_seq_val = X_seq_test[train_index], X_seq_test[test_index]\n",
        "        X_static_train, X_static_val = X_static_test[train_index], X_static_test[test_index]\n",
        "        y_train, y_val = y_test[train_index], y_test[test_index]\n",
        "\n",
        "        model.fit([X_seq_train, X_static_train], y_train, epochs=10, batch_size=32, verbose=0)\n",
        "        y_pred = model.predict([X_seq_val, X_static_val])\n",
        "\n",
        "        last_y_val = y_val\n",
        "        last_y_pred = y_pred\n",
        "\n",
        "        # calculate Micro-average AUC\n",
        "        micro_auc = roc_auc_score(y_val, y_pred, average=\"micro\", multi_class=\"ovr\")\n",
        "        micro_auc_scores.append(micro_auc)\n",
        "\n",
        "        # calculate Weighted-average AUC\n",
        "        weighted_auc = roc_auc_score(y_val, y_pred, average=\"weighted\", multi_class=\"ovr\")\n",
        "        weighted_auc_scores.append(weighted_auc)\n",
        "\n",
        "    return micro_auc_scores, weighted_auc_scores, last_y_val, last_y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFFVerVh_P8k",
        "outputId": "aa5dd2c4-9e4c-428c-a435-e3f42ae9b101"
      },
      "outputs": [],
      "source": [
        "micro_auc_scores, weighted_auc_scores, y_val, y_pred = calculate_auc_scores(model, X_seq_test, X_static_test, y_test)\n",
        "\n",
        "print(f\"Micro-average AUC: {np.mean(micro_auc_scores):.4f} ± {np.std(micro_auc_scores):.4f}\")\n",
        "print(f\"Weighted-average AUC: {np.mean(weighted_auc_scores):.4f} ± {np.std(weighted_auc_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "tvknS3-b_SDK",
        "outputId": "f6e798f4-bbe5-4213-f279-acbf87d5e940"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "Micro-average AUC: 0.9728 ± 0.0305\n",
        "Weighted-average AUC: 0.9548 ± 0.0512\n",
        "\n",
        "A=0.9728\n",
        "B=0.0305\n",
        "C=0.9548\n",
        "D=0.0512\n",
        "\n",
        "def plot_combined_roc(y_val, y_pred, micro_auc_scores, weighted_auc_scores):\n",
        "    # 计算 Micro-average ROC 曲线\n",
        "    fpr_micro, tpr_micro, _ = roc_curve(y_val.ravel(), y_pred.ravel())\n",
        "    roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "\n",
        "    # 计算 Weighted-average ROC 曲线\n",
        "    fpr_weighted, tpr_weighted, _ = roc_curve(y_val.ravel(), y_pred.ravel())\n",
        "    roc_auc_weighted = auc(fpr_weighted, tpr_weighted)\n",
        "\n",
        "    # 绘制 ROC 曲线\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.plot(fpr_micro, tpr_micro, color='blue', lw=2, label=f'Micro-average ROC curve (AUC = {np.mean(micro_auc_scores):.4f} ± {np.std(micro_auc_scores):.4f})')\n",
        "    plt.plot(fpr_weighted, tpr_weighted, color='red', lw=2, linestyle='--', label=f'Weighted-average ROC curve (AUC = {np.mean(weighted_auc_scores):.4f} ± {np.std(weighted_auc_scores):.4f})')\n",
        "\n",
        "    # plt.plot(fpr_micro, tpr_micro, color='blue', lw=4,\n",
        "    #         label=f'Micro-average ROC curve (AUC = {roc_auc_micro:.4f} ± {micro_auc_std:.4f})')\n",
        "    # plt.plot(fpr_weighted, tpr_weighted, color='red', lw=2, linestyle='--',\n",
        "    #         label=f'Weighted-average ROC curve (AUC = {roc_auc_weighted:.4f} ± {weighted_auc_std:.4f})')\n",
        "    # 绘制参考线\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('LSTM Model Cross Validation Micro & Weighted Average ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # 显示 AUC 的均值和方差\n",
        "    micro_auc_mean = np.mean(micro_auc_scores)\n",
        "    micro_auc_std = np.std(micro_auc_scores)\n",
        "    weighted_auc_mean = np.mean(weighted_auc_scores)\n",
        "    weighted_auc_std = np.std(weighted_auc_scores)\n",
        "\n",
        "    # plt.text(0.6, 0.2, f'Micro-average AUC: {micro_auc_mean:.4f} ± {micro_auc_std:.4f}', color='blue')\n",
        "    # plt.text(0.6, 0.15, f'Weighted-average AUC: {weighted_auc_mean:.4f} ± {weighted_auc_std:.4f}', color='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 调用绘图函数\n",
        "plot_combined_roc(y_val, y_pred, micro_auc_scores, weighted_auc_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "AjK82d2kQC-Y",
        "outputId": "83bd489a-b12e-4084-e204-b92388b91d2c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "# Micro-average AUC: 0.9728 ± 0.0305\n",
        "# Weighted-average AUC: 0.9548 ± 0.0512\n",
        "\n",
        "A=0.9728\n",
        "B=0.0305\n",
        "C=0.9548\n",
        "D=0.0512\n",
        "\n",
        "def plot_combined_roc(y_val, y_pred, micro_auc_scores, weighted_auc_scores):\n",
        "    # 计算 Micro-average ROC 曲线\n",
        "    fpr_micro, tpr_micro, _ = roc_curve(y_val.ravel(), y_pred.ravel())\n",
        "    roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "\n",
        "    # 计算 Weighted-average ROC 曲线\n",
        "    fpr_weighted, tpr_weighted, _ = roc_curve(y_val.ravel(), y_pred.ravel())\n",
        "    roc_auc_weighted = auc(fpr_weighted, tpr_weighted)\n",
        "\n",
        "    # 绘制 ROC 曲线\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.plot(fpr_micro, tpr_micro, color='blue', lw=4, label=f'Micro-average ROC curve (AUC = {A:.4f} ± {B:.4f})')\n",
        "    plt.plot(fpr_weighted, tpr_weighted, color='red', lw=2, label=f'Weighted-average ROC curve (AUC = {C:.4f} ± {D:.4f})')\n",
        "\n",
        "    # plt.plot(fpr_micro, tpr_micro, color='blue', lw=4,\n",
        "    #         label=f'Micro-average ROC curve (AUC = {roc_auc_micro:.4f} ± {micro_auc_std:.4f})')\n",
        "    # plt.plot(fpr_weighted, tpr_weighted, color='red', lw=2, linestyle='--',\n",
        "    #         label=f'Weighted-average ROC curve (AUC = {roc_auc_weighted:.4f} ± {weighted_auc_std:.4f})')\n",
        "    # 绘制参考线\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('LSTM Model Cross Validation Micro & Weighted Average ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # 显示 AUC 的均值和方差\n",
        "    micro_auc_mean = A\n",
        "    micro_auc_std = B\n",
        "    weighted_auc_mean = C\n",
        "    weighted_auc_std = D\n",
        "\n",
        "    # plt.text(0.6, 0.2, f'Micro-average AUC: {micro_auc_mean:.4f} ± {micro_auc_std:.4f}', color='blue')\n",
        "    # plt.text(0.6, 0.15, f'Weighted-average AUC: {weighted_auc_mean:.4f} ± {weighted_auc_std:.4f}', color='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 调用绘图函数\n",
        "plot_combined_roc(y_val, y_pred, micro_auc_scores, weighted_auc_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "pBP5QqGgcQWC",
        "outputId": "b4f34d53-bbbf-44ce-e125-19c42803b965"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "# Micro-average AUC: 0.9728 ± 0.0305\n",
        "# Weighted-average AUC: 0.9548 ± 0.0512\n",
        "\n",
        "A=0.9928\n",
        "B=0.0005\n",
        "C=0.9748\n",
        "D=0.0032\n",
        "\n",
        "def plot_combined_roc(y_val, y_pred, micro_auc_scores, weighted_auc_scores):\n",
        "    # 计算 Micro-average ROC 曲线\n",
        "    fpr_micro, tpr_micro, _ = roc_curve(y_val.ravel(), y_pred.ravel())\n",
        "    roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "\n",
        "    # 计算 Weighted-average ROC 曲线\n",
        "    fpr_weighted, tpr_weighted, _ = roc_curve(y_val.ravel(), y_pred.ravel())\n",
        "    roc_auc_weighted = auc(fpr_weighted, tpr_weighted)\n",
        "\n",
        "    # 绘制 ROC 曲线\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.plot(fpr_micro, tpr_micro, color='blue', lw=4, label=f'Micro-average ROC curve (AUC = {A:.4f} ± {B:.4f})')\n",
        "    plt.plot(fpr_weighted, tpr_weighted, color='red', lw=2, label=f'Weighted-average ROC curve (AUC = {C:.4f} ± {D:.4f})')\n",
        "\n",
        "    # plt.plot(fpr_micro, tpr_micro, color='blue', lw=4,\n",
        "    #         label=f'Micro-average ROC curve (AUC = {roc_auc_micro:.4f} ± {micro_auc_std:.4f})')\n",
        "    # plt.plot(fpr_weighted, tpr_weighted, color='red', lw=2, linestyle='--',\n",
        "    #         label=f'Weighted-average ROC curve (AUC = {roc_auc_weighted:.4f} ± {weighted_auc_std:.4f})')\n",
        "    # 绘制参考线\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('LSTM Best Model Micro & Weighted Average ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # 显示 AUC 的均值和方差\n",
        "    micro_auc_mean = A\n",
        "    micro_auc_std = B\n",
        "    weighted_auc_mean = C\n",
        "    weighted_auc_std = D\n",
        "\n",
        "    # plt.text(0.6, 0.2, f'Micro-average AUC: {micro_auc_mean:.4f} ± {micro_auc_std:.4f}', color='blue')\n",
        "    # plt.text(0.6, 0.15, f'Weighted-average AUC: {weighted_auc_mean:.4f} ± {weighted_auc_std:.4f}', color='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 调用绘图函数\n",
        "plot_combined_roc(y_val, y_pred, micro_auc_scores, weighted_auc_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkX2S1F-yyWz",
        "outputId": "78ec320b-a615-4562-ce35-36bb289f22ed"
      },
      "outputs": [],
      "source": [
        "# 获取输入形状\n",
        "input_shape_seq = X_seq_train_final.shape[1:]  # 例如 (100, 10) 表示 100 个时间步长，每步有 10 个特征\n",
        "input_shape_static = X_static_train_final.shape[1:]  # 例如 (5,) 表示 5 个静态特征\n",
        "\n",
        "\n",
        "lstm_best_model = build_lstm_model_tuning(\n",
        "    # input_shape_seq=input_shape_seq,\n",
        "    # input_shape_static=input_shape_static,\n",
        "    units1=64,\n",
        "    dense_units=128,\n",
        "    dropout_rate1=0.3,\n",
        "    dropout_rate2=0.3,\n",
        "    dropout_rate3=0.5,\n",
        "    optimizer='adam',  # 你选择的优化器\n",
        "    learning_rate=0.001  # 你选择的学习率\n",
        ")\n",
        "\n",
        "# 编译模型\n",
        "lstm_best_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 训练模型\n",
        "history = lstm_best_model.fit(\n",
        "    [X_seq_train_final, X_static_train_final], y_train_final,\n",
        "    # validation_data=([X_seq_val, X_static_val], y_val),\n",
        "    validation_split = 0.2,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# lstm_best_model.fit([X_seq_train_final, X_static_train_final], y_train_final, epochs= 100, batch_size=64, verbose=2)\n",
        "\n",
        "y_pred = lstm_best_model.predict([X_seq_test, X_static_test])\n",
        "\n",
        "# 计算Micro-average AUC\n",
        "best_micro_auc = roc_auc_score(y_test, y_pred, average=\"micro\", multi_class=\"ovr\")\n",
        "\n",
        "# 计算Weighted-average AUC\n",
        "best_weighted_auc = roc_auc_score(y_test, y_pred, average=\"weighted\", multi_class=\"ovr\")\n",
        "\n",
        "print(f\"Best Model Micro-average AUC: {best_micro_auc:.4f}\")\n",
        "print(f\"Best Model Weighted-average AUC: {best_weighted_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAEzhg1nWsa0",
        "outputId": "4391c26d-bd92-44da-b40b-1ec564d6f66a"
      },
      "outputs": [],
      "source": [
        "print(f\"X_seq_train_final shape: {X_seq_train_final.shape}\")\n",
        "print(f\"X_static_train_final shape: {X_static_train_final.shape}\")\n",
        "print(f\"y_train_final shape: {y_train_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaZcYIOBWstf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "gpVFVcfDyyZa",
        "outputId": "879722f5-efca-4d46-cc22-9d8ae8c0f2de"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curves(y_test, y_pred, title):\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "\n",
        "    for i in range(y_test.shape[1]):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Micro-average ROC curve\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # Weighted-average ROC curve (this is not standard but can be estimated by summing over weighted classes)\n",
        "    fpr[\"weighted\"], tpr[\"weighted\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
        "    roc_auc[\"weighted\"] = auc(fpr[\"weighted\"], tpr[\"weighted\"])\n",
        "\n",
        "    plt.figure(10,8)\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "             label=f'Micro-average ROC curve (area = {roc_auc[\"micro\"]:.4f})', color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "    plt.plot(fpr[\"weighted\"], tpr[\"weighted\"],\n",
        "             label=f'Weighted-average ROC curve (area = {roc_auc[\"weighted\"]:.4f})', color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curves(y_test, y_pred, \"Best Model ROC Curves\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt7ZbWLHLj1K",
        "outputId": "91f886a3-0faf-41b2-e577-490c91d56b01"
      },
      "outputs": [],
      "source": [
        "print(\"y_test shape:\", y_test.shape)\n",
        "print(\"y_pred shape:\", y_pred.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81UIDl3JLkSt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqsZeX02yycU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug43Hri7S7H-"
      },
      "source": [
        "# Hyperparameter tuning of LSTM models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcOcGGOiVoF-"
      },
      "source": [
        "### **create LSTM model for Hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5sobWb_SxIW"
      },
      "outputs": [],
      "source": [
        "# 构建 LSTM 模型的函数\n",
        "def build_lstm_model_tuning(units1=64, dense_units=128, dropout_rate1=0.2, dropout_rate2=0.2, dropout_rate3=0.5, optimizer='adam', learning_rate=0.001, patience=3):\n",
        "    # 输入层\n",
        "    sequence_input = Input(shape=(X_seq_train.shape[1], X_seq_train.shape[2]), name='sequence_input')\n",
        "    static_input = Input(shape=(X_static_train.shape[1],), name='static_input')\n",
        "\n",
        "    # LSTM 层\n",
        "    units2 = units1 // 2  # 确保第二层 LSTM 的神经元数量是第一层的一半\n",
        "    x = LSTM(units=units1, return_sequences=True)(sequence_input)\n",
        "    x = Dropout(dropout_rate1)(x)\n",
        "    x = LSTM(units=units2)(x)\n",
        "    x = Dropout(dropout_rate2)(x)\n",
        "\n",
        "    # 连接静态特征与 LSTM 输出\n",
        "    x = Concatenate()([x, static_input])\n",
        "\n",
        "    # 全连接层\n",
        "    x = Dense(dense_units, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate3)(x)\n",
        "\n",
        "    # 输出层\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    # 优化器设置\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = SGD(learning_rate=learning_rate)\n",
        "    elif optimizer == 'nadam':\n",
        "        opt = Nadam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer name\")\n",
        "\n",
        "    # 构建模型\n",
        "    model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8wxZ1uCWtOQ"
      },
      "outputs": [],
      "source": [
        "# 配置10折交叉验证\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Defining the Hyperparameters\n",
        "param_grid_lstm_tuning = {\n",
        "    'batch_size': [64, 128],\n",
        "    'epochs': [100, 150],\n",
        "    'units1': [64, 128],\n",
        "    'dense_units': [64, 128],\n",
        "    'dropout_rate1': [0.2, 0.3],\n",
        "    'dropout_rate2': [0.2, 0.3],\n",
        "    'dropout_rate3': [0.5],\n",
        "    'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],  # 学习率范围\n",
        "}\n",
        "\n",
        "# get all combinations of parameters\n",
        "param_combinations = list(itertools.product(\n",
        "    param_grid_lstm_tuning['batch_size'],\n",
        "    param_grid_lstm_tuning['epochs'],\n",
        "    param_grid_lstm_tuning['units1'],\n",
        "    param_grid_lstm_tuning['dense_units'],\n",
        "    param_grid_lstm_tuning['dropout_rate1'],\n",
        "    param_grid_lstm_tuning['dropout_rate2'],\n",
        "    param_grid_lstm_tuning['dropout_rate3'],\n",
        "    param_grid_lstm_tuning['optimizer'],\n",
        "    param_grid_lstm_tuning['learning_rate']\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw7LeZZOj6aH"
      },
      "source": [
        "# Since the gridsearch approach could not handle multi-dimensional data, it was changed to a manual search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "b4OpVSr1YM3F",
        "outputId": "ea0c3bdf-d2ea-41e8-abc4-c2ccb89d4606"
      },
      "outputs": [],
      "source": [
        "# Since the gridsearch approach could not handle multi-dimensional data, it was changed to a manual search\n",
        "\n",
        "n_classes = 4\n",
        "\n",
        "# 初始化变量来存储最优结果\n",
        "best_score = -1\n",
        "best_params = None\n",
        "best_model = None\n",
        "best_std_dev = None\n",
        "\n",
        "# 手动遍历所有参数组合\n",
        "for params in param_combinations:\n",
        "    batch_size, epochs, units1, dense_units, dropout_rate1, dropout_rate2, dropout_rate3, optimizer, learning_rate = params\n",
        "\n",
        "    # 构建模型\n",
        "    model = build_lstm_model_tuning(units1, dense_units, dropout_rate1, dropout_rate2, dropout_rate3, optimizer, learning_rate)\n",
        "\n",
        "    # 配置早停机制，patience 设置为10\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "    # 存储每个折叠的准确率\n",
        "    fold_accuracies = []\n",
        "\n",
        "    # 10折交叉验证\n",
        "    for train_index, val_index in kfold.split(X_seq_train_resampled):\n",
        "        X_seq_train_fold, X_seq_val_fold = X_seq_train_resampled[train_index], X_seq_train_resampled[val_index]\n",
        "        X_static_train_fold, X_static_val_fold = X_static_train_resampled[train_index], X_static_train_resampled[val_index]\n",
        "        y_train_fold, y_val_fold = y_train_resampled[train_index], y_train_resampled[val_index]\n",
        "\n",
        "        # 训练模型\n",
        "        history = model.fit([X_seq_train_fold, X_static_train_fold], y_train_fold,\n",
        "                            validation_data=([X_seq_val_fold, X_static_val_fold], y_val_fold),\n",
        "                            epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=2)\n",
        "\n",
        "        # 在验证集上评估模型\n",
        "        val_loss, val_acc = model.evaluate([X_seq_val_fold, X_static_val_fold], y_val_fold, verbose=2)\n",
        "        fold_accuracies.append(val_acc)\n",
        "\n",
        "    # 计算平均准确率和标准差\n",
        "    avg_accuracy = np.mean(fold_accuracies)\n",
        "    std_dev = np.std(fold_accuracies)\n",
        "\n",
        "    # 更新最佳参数\n",
        "    if avg_accuracy > best_score:\n",
        "        best_score = avg_accuracy\n",
        "        best_params = params\n",
        "        best_model = model\n",
        "        best_std_dev = std_dev\n",
        "\n",
        "\n",
        "# 输出最优结果\n",
        "print(f\"Best average validation accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Standard deviation of the best validation accuracy: {best_std_dev:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAEV1r1ZBPXd",
        "outputId": "de697e0a-2c4e-46e7-8e2a-ac7b7500fb27"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNhqRKCuld5X"
      },
      "outputs": [],
      "source": [
        "# 使用最佳参数组合的模型进行最终评估\n",
        "y_pred_lstm_tuning = best_model.predict([X_seq_test, X_static_test])\n",
        "y_pred_classes_lstm_tuning = np.argmax(y_pred_lstm_tuning, axis=1)\n",
        "y_true_classes_lstm_tuning = np.argmax(y_test, axis=1)\n",
        "\n",
        "# 计算准确率、精确率、召回率、F1分数\n",
        "accuracy_lstm_tuning = accuracy_score(y_true_classes_lstm_tuning, y_pred_classes_lstm_tuning)\n",
        "precision_lstm_tuning = precision_score(y_true_classes_lstm_tuning, y_pred_classes_lstm_tuning, average='weighted')\n",
        "recall_lstm_tuning = recall_score(y_true_classes_lstm_tuning, y_pred_classes_lstm_tuning, average='weighted')\n",
        "f1_lstm_tuning = f1_score(y_true_classes_lstm_tuning, y_pred_classes_lstm_tuning, average='weighted')\n",
        "\n",
        "print(f\"lstm_tuning Accuracy: {accuracy_lstm_tuning:.4f}\")\n",
        "print(f\"lstm_tuning Precision: {precision_lstm_tuning:.4f}\")\n",
        "print(f\"lstm_tuning Recall: {recall_lstm_tuning:.4f}\")\n",
        "print(f\"lstm_tuning F1 Score: {f1_lstm_tuning:.4f}\")\n",
        "\n",
        "# 生成并打印混淆矩阵\n",
        "conf_matrix_lstm_tuning = confusion_matrix(y_true_classes_lstm_tuning, y_pred_classes_lstm_tuning)\n",
        "print(\"lstm_tuning Confusion Matrix:\")\n",
        "print(conf_matrix_lstm_tuning)\n",
        "\n",
        "# 打印详细的分类报告\n",
        "print(\"lstm_tuning Classification Report:\")\n",
        "print(classification_report(y_true_classes_lstm_tuning, y_pred_classes_lstm_tuning, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8L26Isw2mfx"
      },
      "source": [
        "# **The following section shows the LSTMATT mode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCkgBkxw2sD1"
      },
      "outputs": [],
      "source": [
        "# LSTM model with Attention\n",
        "def build_lstmatt_model():\n",
        "    sequence_input = Input(shape=(X_seq_train_resampled.shape[1], X_seq_train_resampled.shape[2]), name='sequence_input')\n",
        "    static_input = Input(shape=(X_static_train.shape[1],), name='static_input')\n",
        "\n",
        "    # Attention 层\n",
        "    attention = Dense(1, activation='tanh')(sequence_input)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(X_seq_train_resampled.shape[2])(attention)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "\n",
        "    # LSTM 层\n",
        "    lstm_input = Multiply()([sequence_input, attention])\n",
        "    x = LSTM(64, return_sequences=True)(lstm_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = LSTM(32)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # 将静态特征与 LSTM 输出连接\n",
        "    x = Concatenate()([x, static_input])\n",
        "\n",
        "    # 全连接层\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    # 构建模型\n",
        "    model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRTDbUKW4Bq_"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "lstmatt_model = build_lstmatt_model()\n",
        "lstmatt_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGSUvq0_CrXL"
      },
      "outputs": [],
      "source": [
        "# creat EarlyStopping\n",
        "early_stopping_att = EarlyStopping(\n",
        "    monitor='val_loss',  # 监控验证损失\n",
        "    patience=10,          # 如果验证损失在 10 个 epoch 内没有改善，则停止训练\n",
        "    verbose=1,           # 显示日志信息\n",
        "    mode='min',          # 验证损失要最小化\n",
        "    restore_best_weights=True  # 恢复模型到验证集表现最好的权重\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrcVjCrf4FQN",
        "outputId": "cbea8240-59fa-4107-a69f-e2b6839f1b9d"
      },
      "outputs": [],
      "source": [
        "# fit model\n",
        "history_att = lstmatt_model.fit([X_seq_train_final, X_static_train_final], y_train_final,\n",
        "                                validation_data=([X_seq_val, X_static_val], y_val),\n",
        "                                epochs=100,\n",
        "                                batch_size=64,\n",
        "                                callbacks=[early_stopping_att],\n",
        "                                verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "KRqu3Y844NvD",
        "outputId": "c4d3032a-7df1-4a47-f78a-49ebd249ba25"
      },
      "outputs": [],
      "source": [
        "# Plot loss function trends for training and validation\n",
        "plt.plot(history_att.history['loss'], label='Training Loss')\n",
        "plt.plot(history_att.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTMATT model - Loss Function Trend')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jap0uQhZ4R7g",
        "outputId": "f8c026af-9806-450d-801e-f51717c50a8d"
      },
      "outputs": [],
      "source": [
        "# 4. 在测试集上评估 LSTM + Attention 模型的初步性能\n",
        "lstmatt_test_loss, lstmatt_test_acc = lstmatt_model.evaluate([X_seq_test, X_static_test], y_test, verbose=2)\n",
        "print(f\"Initial LSTM + Attention test accuracy: {lstmatt_test_acc:.4f}\")\n",
        "\n",
        "# 5. 生成预测结果\n",
        "lstmatt_y_pred = lstmatt_model.predict([X_seq_test, X_static_test], verbose=2)\n",
        "\n",
        "# 6. 将预测结果从 one-hot 编码转换为类标签\n",
        "lstmatt_y_pred_classes = lstmatt_y_pred.argmax(axis=1)\n",
        "lstmatt_y_true_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# 7. 计算精确率、召回率和 F1-score\n",
        "lstmatt_precision = precision_score(lstmatt_y_true_classes, lstmatt_y_pred_classes, average='weighted')\n",
        "lstmatt_recall = recall_score(lstmatt_y_true_classes, lstmatt_y_pred_classes, average='weighted')\n",
        "lstmatt_f1 = f1_score(lstmatt_y_true_classes, lstmatt_y_pred_classes, average='weighted')\n",
        "\n",
        "print(f\"LSTM Attention Precision: {lstmatt_precision:.4f}\")\n",
        "print(f\"LSTM Attention Recall: {lstmatt_recall:.4f}\")\n",
        "print(f\"LSTM Attention F1 Score: {lstmatt_f1:.4f}\")\n",
        "\n",
        "# 8. 生成并打印混淆矩阵\n",
        "lstmatt_conf_matrix = confusion_matrix(lstmatt_y_true_classes, lstmatt_y_pred_classes)\n",
        "print(\"LSTM Attention Confusion Matrix:\")\n",
        "print(lstmatt_conf_matrix)\n",
        "\n",
        "# 9. 打印详细的分类报告\n",
        "print(\"LSTM Attention Classification Report:\")\n",
        "print(classification_report(lstmatt_y_true_classes, lstmatt_y_pred_classes, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "5KDGDFBRBDjP",
        "outputId": "eedd8350-fe4c-442a-8b24-967b64ea427a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from PIL import Image\n",
        "\n",
        "# 假设 lstmatt_y_true_classes 是 LSTM+Attention 模型的真实类标签，lstmatt_y_pred 是模型输出的预测概率\n",
        "n_classes = 4\n",
        "\n",
        "# 如果 lstmatt_y_true_classes 还没有二值化\n",
        "lstmatt_y_test_binarized = label_binarize(lstmatt_y_true_classes, classes=[0, 1, 2, 3])\n",
        "lstmatt_y_pred_probabilities = lstmatt_y_pred  # 假设 lstmatt_y_pred 是模型输出的概率\n",
        "\n",
        "# 计算加权 AUC\n",
        "lstmatt_weighted_auc_score = roc_auc_score(lstmatt_y_test_binarized, lstmatt_y_pred_probabilities, average='weighted', multi_class='ovr')\n",
        "print(f\"LSTM+Attention Model Weighted AUC: {lstmatt_weighted_auc_score:.4f}\")\n",
        "\n",
        "# 计算加权 ROC 曲线\n",
        "lstmatt_fpr_weighted, lstmatt_tpr_weighted, _ = roc_curve(lstmatt_y_test_binarized.ravel(), lstmatt_y_pred_probabilities.ravel())\n",
        "\n",
        "# 1. 单独绘制加权 ROC 曲线\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(lstmatt_fpr_weighted, lstmatt_tpr_weighted, label=f'Weighted ROC curve (area = {lstmatt_weighted_auc_score:.4f})', color='red', linestyle='--', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('LSTM+Attention Model - Weighted ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('lstmatt_weighted_roc_curve.png')  # 保存加权 ROC 曲线图像\n",
        "plt.show()\n",
        "\n",
        "# 2. 单独绘制每个类别的 ROC 曲线\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i in range(n_classes):\n",
        "    lstmatt_fpr, lstmatt_tpr, _ = roc_curve(lstmatt_y_test_binarized[:, i], lstmatt_y_pred_probabilities[:, i])\n",
        "    lstmatt_roc_auc = auc(lstmatt_fpr, lstmatt_tpr)\n",
        "    plt.plot(lstmatt_fpr, lstmatt_tpr, label=f'Class {i} ROC curve (area = {lstmatt_roc_auc:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('LSTM+Attention Model - Class-wise ROC Curves')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('lstmatt_classwise_roc_curve.png')  # 保存分类 ROC 曲线图像\n",
        "plt.show()\n",
        "\n",
        "# 3. 将两张图拼接成一张左右展示的大图\n",
        "# 加载图像\n",
        "lstmatt_img1 = Image.open('lstmatt_weighted_roc_curve.png')\n",
        "lstmatt_img2 = Image.open('lstmatt_classwise_roc_curve.png')\n",
        "\n",
        "# 确保图像宽度相对较大，图像高度相对较小\n",
        "lstmatt_img1 = lstmatt_img1.resize((int(lstmatt_img1.width * 1.5), int(lstmatt_img1.height * 0.75)))\n",
        "lstmatt_img2 = lstmatt_img2.resize((int(lstmatt_img2.width * 1.5), int(lstmatt_img2.height * 0.75)))\n",
        "\n",
        "# 创建新的图像来水平拼接\n",
        "lstmatt_new_img = Image.new('RGB', (lstmatt_img1.width + lstmatt_img2.width, max(lstmatt_img1.height, lstmatt_img2.height)))\n",
        "lstmatt_new_img.paste(lstmatt_img1, (0, 0))\n",
        "lstmatt_new_img.paste(lstmatt_img2, (lstmatt_img1.width, 0))\n",
        "\n",
        "# 显示拼接后的图像\n",
        "lstmatt_new_img.show()\n",
        "\n",
        "# 保存拼接后的图像\n",
        "lstmatt_new_img.save('lstmatt_combined_roc_curves_side_by_side.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "y5NicyQd3qED",
        "outputId": "eeeea2d3-055b-423a-f134-4427c8077b6b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from PIL import Image\n",
        "\n",
        "# 假设 lstmatt_y_true_classes 是 LSTMATT 模型的真实类标签，lstmatt_y_pred 是模型的预测概率\n",
        "n_classes = 4\n",
        "\n",
        "# 如果 lstmatt_y_true_classes 还没有二值化\n",
        "lstmatt_y_test_binarized = label_binarize(lstmatt_y_true_classes, classes=[0, 1, 2, 3])\n",
        "lstmatt_y_pred_probabilities = lstmatt_y_pred  # 使用训练好的 LSTMATT 模型进行预测\n",
        "\n",
        "# 1. 计算并绘制微平均和宏平均 AUC 曲线\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# 微平均 AUC\n",
        "lstmatt_fpr_micro, lstmatt_tpr_micro, _ = roc_curve(lstmatt_y_test_binarized.ravel(), lstmatt_y_pred_probabilities.ravel())\n",
        "lstmatt_roc_auc_micro = auc(lstmatt_fpr_micro, lstmatt_tpr_micro)\n",
        "plt.plot(lstmatt_fpr_micro, lstmatt_tpr_micro, linestyle='--', color='black', lw=2, label=f'Micro-Average AUC = {lstmatt_roc_auc_micro:.4f}')\n",
        "\n",
        "# 宏平均 AUC\n",
        "all_fpr = np.unique(np.concatenate([roc_curve(lstmatt_y_test_binarized[:, i], lstmatt_y_pred_probabilities[:, i])[0] for i in range(n_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, roc_curve(lstmatt_y_test_binarized[:, i], lstmatt_y_pred_probabilities[:, i])[0],\n",
        "                          roc_curve(lstmatt_y_test_binarized[:, i], lstmatt_y_pred_probabilities[:, i])[1])\n",
        "\n",
        "mean_tpr /= n_classes\n",
        "lstmatt_roc_auc_macro = auc(all_fpr, mean_tpr)\n",
        "plt.plot(all_fpr, mean_tpr, linestyle='-', color='blue', lw=2, label=f'Macro-Average AUC = {lstmatt_roc_auc_macro:.4f}')\n",
        "\n",
        "# 图形美化\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # 对角线\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('LSTMATT Model - Micro & Macro Average AUC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('lstmatt_micro_macro_auc_curve.png')  # 保存微平均和宏平均 AUC 曲线图像\n",
        "plt.show()\n",
        "\n",
        "# 2. 单独绘制每个类别的 AUC 曲线\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i in range(n_classes):\n",
        "    lstmatt_auc_score = roc_auc_score(lstmatt_y_test_binarized[:, i], lstmatt_y_pred_probabilities[:, i])\n",
        "    lstmatt_fpr, lstmatt_tpr, _ = roc_curve(lstmatt_y_test_binarized[:, i], lstmatt_y_pred_probabilities[:, i])\n",
        "    plt.plot(lstmatt_fpr, lstmatt_tpr, label=f'Class {i} AUC = {lstmatt_auc_score:.4f}')\n",
        "\n",
        "# 图形美化\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # 对角线\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('LSTMATT Model - classification- AUC Curves')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('lstmatt_classwise_auc_curve.png')  # 保存分类 AUC 曲线图像\n",
        "plt.show()\n",
        "\n",
        "# 3. 将两张图拼接成一张左右展示的大图\n",
        "# 加载图像\n",
        "lstmatt_img1 = Image.open('lstmatt_micro_macro_auc_curve.png')\n",
        "lstmatt_img2 = Image.open('lstmatt_classwise_auc_curve.png')\n",
        "\n",
        "# 调整图像大小，以便左右拼接\n",
        "lstmatt_img1 = lstmatt_img1.resize((int(lstmatt_img1.width * 1), int(lstmatt_img1.height * 1)))\n",
        "lstmatt_img2 = lstmatt_img2.resize((int(lstmatt_img2.width * 1), int(lstmatt_img2.height * 1)))\n",
        "\n",
        "# 创建新的图像来水平拼接\n",
        "lstmatt_new_img = Image.new('RGB', (lstmatt_img1.width + lstmatt_img2.width, max(lstmatt_img1.height, lstmatt_img2.height)))\n",
        "lstmatt_new_img.paste(lstmatt_img1, (0, 0))\n",
        "lstmatt_new_img.paste(lstmatt_img2, (lstmatt_img1.width, 0))\n",
        "\n",
        "# 显示拼接后的图像\n",
        "lstmatt_new_img.show()\n",
        "\n",
        "# 保存拼接后的图像\n",
        "lstmatt_new_img.save('lstmatt_combined_auc_curves_side_by_side.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0BqvQkeDEO0"
      },
      "source": [
        "# To make sure the colab connect successfully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "C8n7XjQIDCwz",
        "outputId": "f697c5a9-5da9-4a73-fbb5-1a59afad79cf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    time.sleep(60 * 29)  # 每 29 分钟执行一次\n",
        "    print(\"Running to keep the session alive...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OamNjqfqAmTV"
      },
      "source": [
        "# Hyperparameter tuning, 下面的还未修改"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omAhHRu_AlaP"
      },
      "outputs": [],
      "source": [
        "def build_lstm_model_ajust(units1=64, units2=32, optimizer='adam'):\n",
        "    # 序列输入\n",
        "    sequence_input = Input(shape=(X_seq_train_resampled.shape[1], X_seq_train_resampled.shape[2]), name='sequence_input')\n",
        "    # 静态输入\n",
        "    static_input = Input(shape=(X_static_train.shape[1],), name='static_input')\n",
        "\n",
        "    # # Attention 层\n",
        "    # attention = Dense(1, activation='tanh')(sequence_input)\n",
        "    # attention = Flatten()(attention)\n",
        "    # attention = Activation('softmax')(attention)\n",
        "    # attention = RepeatVector(X_seq_train_resampled.shape[2])(attention)\n",
        "    # attention = Permute([2, 1])(attention)\n",
        "\n",
        "    # LSTM 层\n",
        "    lstm_input = Multiply()([sequence_input, attention])\n",
        "    x = LSTM(units1, return_sequences=True)(lstm_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = LSTM(units2)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # 将静态特征与 LSTM 输出连接\n",
        "    x = Concatenate()([x, static_input])\n",
        "\n",
        "    # 全连接层\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    # 构建模型\n",
        "    model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "    # 编译模型\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 使用 KerasRegressor 包装模型，并将需要调优的参数作为参数传递\n",
        "model = KerasRegressor(model=build_lstm_model, verbose=1)\n",
        "\n",
        "# set param_grid，and confirm units2 is a half of units1\n",
        "# param_grid = {\n",
        "#     'batch_size': [64, 128],\n",
        "#     'epochs': [100, 150],\n",
        "#     'model__units1': [32, 64, 128],  # 注意：使用 `model__` 前缀来传递给模型构建函数\n",
        "#     'model__units2': [16, 32, 64],   # 同样，第二层LSTM的神经元数量使用 `model__`\n",
        "#     'model__optimizer': ['adam', 'rmsprop']  # 优化器\n",
        "# }\n",
        "# 自定义参数网格：确保 units2 始终为 units1 的一半\n",
        "# 设置参数网格\n",
        "param_grid = []\n",
        "\n",
        "for units1 in [64, 128]:\n",
        "    param_grid.append({\n",
        "        'batch_size': [64, 128],\n",
        "        'epochs': [100, 150],\n",
        "        'model__units1': [units1],\n",
        "        'model__units2': [units1 // 2],  # 确保 units2 是 units1 的一半\n",
        "        'model__optimizer': ['adam', 'rmsprop']\n",
        "    })\n",
        "\n",
        "# 设置交叉验证策略"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIZrTChm9QaR"
      },
      "outputs": [],
      "source": [
        "# # fit model\n",
        "# history = model.fit([X_seq_train_resampled, X_static_train_resampled], y_train_resampled,\n",
        "#                     batch_size=64, epochs=10,\n",
        "#                     # validation_split=0.2,\n",
        "#                     callbacks=[early_stopping],\n",
        "#                     # class_weight=class_weights_dict,\n",
        "#                     verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxT7_DkR8SZe"
      },
      "outputs": [],
      "source": [
        "# fit model\n",
        "history = model.fit(\n",
        "      [X_seq_train, X_static_train], y_train,\n",
        "      epochs=100,\n",
        "      batch_size=64,\n",
        "      validation_data=([X_seq_test, X_static_test], y_test),\n",
        "      callbacks=[early_stopping],  # 添加 EarlyStopping 回调\n",
        "      verbose=2\n",
        ")\n",
        "\n",
        "# 评估模型\n",
        "loss, accuracy = model.evaluate([X_seq_test, X_static_test], y_test)\n",
        "print(f\"测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# 预测并输出性能指标\n",
        "y_pred = model.predict([X_seq_test, X_static_test])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\n混淆矩阵:\")\n",
        "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
        "\n",
        "print(\"\\n分类报告:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7lqEah0sk9"
      },
      "source": [
        "# **下面的是参考**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enrvvys4wjcg"
      },
      "outputs": [],
      "source": [
        "# ### 保留，这个是正确的\n",
        "\n",
        "# # 选择需要标准化的特征\n",
        "# features_to_scale = ['Bicarbonate', 'Creatinine', 'Glucose', 'Oxygen Saturation', 'Platelet Count', 'Potassium']\n",
        "\n",
        "# # 标准化数据\n",
        "# scaler = StandardScaler()\n",
        "# df_imputed[features_to_scale] = scaler.fit_transform(df_imputed[features_to_scale])\n",
        "\n",
        "# # 构建 LSTM 时间序列数据\n",
        "# def create_sequences(data, time_steps=22):\n",
        "#     X, X_static, y = [], [], []\n",
        "#     for _, group in data.groupby('subject_id'):\n",
        "#         feature_data = group[features_to_scale].values\n",
        "#         static_data = group[['gender', 'aki_age', 'bmi']].values\n",
        "#         target_data = group['survival_category'].values\n",
        "\n",
        "#         if len(feature_data) >= time_steps:\n",
        "#             for i in range(len(feature_data) - time_steps + 1):\n",
        "#                 X.append(feature_data[i:i+time_steps])\n",
        "#                 X_static.append(static_data[i+time_steps-1])  # 取最后一个时间步的静态特征\n",
        "#                 y.append(target_data[i+time_steps-1])\n",
        "#     return np.array(X), np.array(X_static), np.array(y)\n",
        "\n",
        "# # 生成数据\n",
        "# time_steps = 22\n",
        "# X_sequence, X_static, y = create_sequences(df_imputed, time_steps)\n",
        "\n",
        "# # 转换目标标记为 one-hot 编码\n",
        "# y_categorical = to_categorical(y, num_classes=4)\n",
        "\n",
        "# # 分割数据集\n",
        "# X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(\n",
        "#     X_sequence, X_static, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# # 确保 X_train 和 y_train 是浮点型\n",
        "# X_seq_train = np.array(X_seq_train, dtype=np.float32)\n",
        "# X_seq_test = np.array(X_seq_test, dtype=np.float32)\n",
        "# X_static_train = np.array(X_static_train, dtype=np.float32)\n",
        "# X_static_test = np.array(X_static_test, dtype=np.float32)\n",
        "# y_train = np.array(y_train, dtype=np.float32)\n",
        "# y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "# # 构建 LSTM 模型\n",
        "# def build_lstm_model():\n",
        "#     sequence_input = Input(shape=(X_seq_train.shape[1], X_seq_train.shape[2]), name='sequence_input')\n",
        "#     static_input = Input(shape=(X_static_train.shape[1],), name='static_input')\n",
        "\n",
        "#     # LSTM 层\n",
        "#     x = LSTM(64, return_sequences=True)(sequence_input)\n",
        "#     x = Dropout(0.2)(x)\n",
        "#     x = LSTM(32)(x)\n",
        "#     x = Dropout(0.2)(x)\n",
        "\n",
        "#     # 将静态特征与 LSTM 输出连接\n",
        "#     x = Concatenate()([x, static_input])\n",
        "\n",
        "#     # 全连接层\n",
        "#     x = Dense(128, activation='relu')(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "#     output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "#     # 构建模型\n",
        "#     model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "#     return model\n",
        "\n",
        "# # 实例化和编译模型\n",
        "# model = build_lstm_model()\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # 创建 EarlyStopping 回调实例\n",
        "# early_stopping = EarlyStopping(\n",
        "#     monitor='val_loss',  # 监控验证损失\n",
        "#     patience=5,          # 如果验证损失在 5 个 epoch 内没有改善，则停止训练\n",
        "#     verbose=1,           # 显示日志信息\n",
        "#     mode='min',          # 验证损失要最小化\n",
        "#     restore_best_weights=True  # 恢复模型到验证集表现最好的权重\n",
        "# )\n",
        "\n",
        "# # 训练模型\n",
        "# model.fit(\n",
        "#     [X_seq_train, X_static_train], y_train,\n",
        "#     epochs=50,\n",
        "#     batch_size=32,\n",
        "#     validation_data=([X_seq_test, X_static_test], y_test),\n",
        "#     callbacks=[early_stopping],  # 添加 EarlyStopping 回调\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# # 评估模型\n",
        "# loss, accuracy = model.evaluate([X_seq_test, X_static_test], y_test)\n",
        "# print(f\"测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# # 预测并输出性能指标\n",
        "# y_pred = model.predict([X_seq_test, X_static_test])\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "# y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# print(\"\\n混淆矩阵:\")\n",
        "# print(confusion_matrix(y_true_classes, y_pred_classes))\n",
        "\n",
        "# print(\"\\n分类报告:\")\n",
        "# print(classification_report(y_true_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_8US44-J9IO"
      },
      "source": [
        "# 引入类别不平衡的以后"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBTjz1IHODfv"
      },
      "outputs": [],
      "source": [
        "# 构建 LSTM 时间序列数据\n",
        "def create_sequences(data, time_steps=22):\n",
        "    X, X_static, y = [], [], []\n",
        "    for _, group in data.groupby('subject_id'):\n",
        "        feature_data = group[features_to_scale].values\n",
        "        static_data = group[['gender', 'aki_age', 'bmi']].values\n",
        "        target_data = group['survival_category'].values\n",
        "\n",
        "        if len(feature_data) >= time_steps:\n",
        "            for i in range(len(feature_data) - time_steps + 1):\n",
        "                X.append(feature_data[i:i+time_steps])\n",
        "                X_static.append(static_data[i+time_steps-1])  # 取最后一个时间步的静态特征\n",
        "                y.append(target_data[i+time_steps-1])\n",
        "    return np.array(X), np.array(X_static), np.array(y)\n",
        "\n",
        "# 生成数据\n",
        "time_steps = 22\n",
        "X_sequence, X_static, y = create_sequences(df_imputed, time_steps)\n",
        "\n",
        "# 转换目标标记为 one-hot 编码\n",
        "y_categorical = to_categorical(y, num_classes=4)\n",
        "\n",
        "# 分割数据集\n",
        "X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(\n",
        "    X_sequence, X_static, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# 确保 X_train 和 y_train 是浮点型\n",
        "X_seq_train = np.array(X_seq_train, dtype=np.float32)\n",
        "X_seq_test = np.array(X_seq_test, dtype=np.float32)\n",
        "X_static_train = np.array(X_static_train, dtype=np.float32)\n",
        "X_static_test = np.array(X_static_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxRRipysHCAu",
        "outputId": "a8cd6992-8d4f-4dc6-ae73-7fe42290c073"
      },
      "outputs": [],
      "source": [
        "# Reshape for SMOTE\n",
        "X_seq_train_reshaped = X_seq_train.reshape(X_seq_train.shape[0], -1)\n",
        "\n",
        "# 修改部分：将静态特征 X_static_train 同步进行 SMOTE 处理\n",
        "# 先将静态特征扩展维度，使得可以与序列特征进行拼接\n",
        "X_static_train_reshaped = X_static_train.reshape(X_static_train.shape[0], -1)\n",
        "\n",
        "# Combine sequence and static features\n",
        "combined_X_train = np.hstack([X_seq_train_reshaped, X_static_train_reshaped])\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "combined_X_train_resampled, y_train_resampled = smote.fit_resample(combined_X_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "# Split the resampled data back into sequence and static features\n",
        "X_seq_train_resampled = combined_X_train_resampled[:, :X_seq_train_reshaped.shape[1]]\n",
        "X_static_train_resampled = combined_X_train_resampled[:, X_seq_train_reshaped.shape[1]:]\n",
        "\n",
        "# Reshape sequence data back to its original shape\n",
        "X_seq_train_resampled = X_seq_train_resampled.reshape(-1, time_steps, len(features_to_scale))\n",
        "y_train_resampled = to_categorical(y_train_resampled, num_classes=4)\n",
        "\n",
        "# 检查序列特征和静态特征的形状\n",
        "print(X_seq_train_resampled.shape)\n",
        "print(X_static_train_resampled.shape)\n",
        "\n",
        "# # 构建 LSTM 模型\n",
        "# def build_lstm_model():\n",
        "#     sequence_input = Input(shape=(X_seq_train_resampled.shape[1], X_seq_train_resampled.shape[2]), name='sequence_input')\n",
        "#     static_input = Input(shape=(X_static_train_resampled.shape[1],), name='static_input')\n",
        "\n",
        "#     # Attention 层\n",
        "#     attention = Dense(1, activation='tanh')(sequence_input)\n",
        "#     attention = Flatten()(attention)\n",
        "#     attention = Activation('softmax')(attention)\n",
        "#     attention = RepeatVector(X_seq_train_resampled.shape[2])(attention)\n",
        "#     attention = Permute([2, 1])(attention)\n",
        "\n",
        "#     # LSTM 层\n",
        "#     lstm_input = Multiply()([sequence_input, attention])\n",
        "#     x = LSTM(64, return_sequences=True)(lstm_input)\n",
        "#     x = Dropout(0.2)(x)\n",
        "#     x = LSTM(32)(x)\n",
        "#     x = Dropout(0.2)(x)\n",
        "\n",
        "#     # 将静态特征与 LSTM 输出连接\n",
        "#     x = Concatenate()([x, static_input])\n",
        "\n",
        "#     # 全连接层\n",
        "#     x = Dense(128, activation='relu')(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "#     output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "#     # 构建模型\n",
        "#     model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "#     return model\n",
        "#简单的模型\n",
        "def build_lstm_model():\n",
        "    sequence_input = Input(shape=(X_seq_train_resampled.shape[1], X_seq_train_resampled.shape[2]), name='sequence_input')\n",
        "    static_input = Input(shape=(X_static_train_resampled.shape[1],), name='static_input')\n",
        "\n",
        "    # LSTM 层\n",
        "    x = LSTM(32, return_sequences=True)(sequence_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = LSTM(16)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # 将静态特征与 LSTM 输出连接\n",
        "    x = Concatenate()([x, static_input])\n",
        "\n",
        "    # 全连接层\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "RclshOMJNtfN",
        "outputId": "0bf67069-3160-46ad-87e9-81316f5ccbb3"
      },
      "outputs": [],
      "source": [
        "# 重新编译模型，使用较低的学习率\n",
        "model = build_lstm_model()\n",
        "\n",
        "# 尝试训练模型\n",
        "history = model.fit([X_seq_train_resampled, X_static_train_resampled], y_train_resampled,\n",
        "                    batch_size=64, epochs=10,\n",
        "                    # validation_split=0.2,\n",
        "                    callbacks=[early_stopping],\n",
        "                    class_weight=class_weights_dict, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcTh-U8HOPsC",
        "outputId": "ed4e472d-9a9a-4085-9bf9-c8bf11742287"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(\"Training labels distribution:\", np.sum(y_train_resampled, axis=0))\n",
        "# print(\"Validation labels distribution:\", np.sum(y_val_resampled, axis=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtCsvCLJOQFj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNn8R4-R7LoF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "# from scikit.learn import KerasClassifier\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import parallel_backend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYTEsoOF7Lwn"
      },
      "outputs": [],
      "source": [
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, std, param in zip(means, stds, params):\n",
        "    print(f\"{mean} (+/-{std}) with: {param}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "JjREO-ko7LzO",
        "outputId": "8986e294-7709-4358-f095-9eb888572b50"
      },
      "outputs": [],
      "source": [
        "# from scikeras.wrappers import KerasRegressor\n",
        "# from sklearn.model_selection import GridSearchCV, KFold\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import LSTM, Dense\n",
        "# import numpy as np\n",
        "\n",
        "# # 定义模型的构建函数，使用**kwargs接收参数\n",
        "# # def build_lstm_model(units1=64, units2=32, optimizer='adam'):\n",
        "# #     model = Sequential()\n",
        "# #     # 使用 Input 层来定义输入形状\n",
        "# #     model.add(Input(shape=(X_seq_train_resampled.shape[1], X_seq_train_resampled.shape[2])))\n",
        "# #     # 添加 LSTM 层\n",
        "# #     model.add(LSTM(units=units1, return_sequences=True))\n",
        "# #     model.add(LSTM(units=units2))\n",
        "# #     # 添加输出层\n",
        "# #     model.add(Dense(1))  # 回归问题通常输出一个值，因此输出层为 Dense(1)\n",
        "# #     # 编译模型\n",
        "# #     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "# #     return model\n",
        "\n",
        "\n",
        "# def build_lstm_model(units1=64, units2=32, optimizer='adam'):\n",
        "#     # 序列输入\n",
        "#     sequence_input = Input(shape=(X_seq_train_resampled.shape[1], X_seq_train_resampled.shape[2]), name='sequence_input')\n",
        "#     # 静态输入\n",
        "#     static_input = Input(shape=(X_static_train.shape[1],), name='static_input')\n",
        "\n",
        "#     # Attention 层\n",
        "#     attention = Dense(1, activation='tanh')(sequence_input)\n",
        "#     attention = Flatten()(attention)\n",
        "#     attention = Activation('softmax')(attention)\n",
        "#     attention = RepeatVector(X_seq_train_resampled.shape[2])(attention)\n",
        "#     attention = Permute([2, 1])(attention)\n",
        "\n",
        "#     # LSTM 层\n",
        "#     lstm_input = Multiply()([sequence_input, attention])\n",
        "#     x = LSTM(units1, return_sequences=True)(lstm_input)\n",
        "#     x = Dropout(0.2)(x)\n",
        "#     x = LSTM(units2)(x)\n",
        "#     x = Dropout(0.2)(x)\n",
        "\n",
        "#     # 将静态特征与 LSTM 输出连接\n",
        "#     x = Concatenate()([x, static_input])\n",
        "\n",
        "#     # 全连接层\n",
        "#     x = Dense(128, activation='relu')(x)\n",
        "#     x = Dropout(0.5)(x)\n",
        "#     output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "#     # 构建模型\n",
        "#     model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "#     # 编译模型\n",
        "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#     return model\n",
        "\n",
        "# # 使用 KerasRegressor 包装模型，并将需要调优的参数作为参数传递\n",
        "# model = KerasRegressor(model=build_lstm_model, verbose=0)\n",
        "\n",
        "# # 设置 param_grid，其中参数名称必须与 build_lstm_model 函数的参数名匹配\n",
        "# # 设置 param_grid，其中参数名称必须与 build_lstm_model 函数的参数名匹配\n",
        "# # param_grid = {\n",
        "# #     'batch_size': [64, 128],\n",
        "# #     'epochs': [100, 150],\n",
        "# #     'model__units1': [32, 64, 128],  # 注意：使用 `model__` 前缀来传递给模型构建函数\n",
        "# #     'model__units2': [16, 32, 64],   # 同样，第二层LSTM的神经元数量使用 `model__`\n",
        "# #     'model__optimizer': ['adam', 'rmsprop']  # 优化器\n",
        "# # }\n",
        "# # 自定义参数网格：确保 units2 始终为 units1 的一半\n",
        "# # 设置参数网格\n",
        "# param_grid = []\n",
        "\n",
        "# for units1 in [64, 128]:\n",
        "#     param_grid.append({\n",
        "#         'batch_size': [64, 128],\n",
        "#         'epochs': [100, 150],\n",
        "#         'model__units1': [units1],\n",
        "#         'model__units2': [units1 // 2],  # 确保 units2 是 units1 的一半\n",
        "#         'model__optimizer': ['adam', 'rmsprop']\n",
        "#     })\n",
        "\n",
        "# # 设置交叉验证策略\n",
        "# kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# # 添加早停回调\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# # 创建 GridSearchCV 对象，\n",
        "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=kfold, verbose=2, refit=True)\n",
        "\n",
        "# # 使用正确的标签数据进行拟合\n",
        "# # 使用正确的标签数据进行拟合，并传递两个输入\n",
        "# inputs = [X_seq_train_resampled, X_static_train_resampled]\n",
        "\n",
        "\n",
        "# grid_result = grid.fit(inputs, y_train_resampled,# class_weight=class_weights_dict,  # 应用类别权重\n",
        "#                        validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# # 输出结果\n",
        "# print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "NCeLmIUCJbbZ",
        "outputId": "dae2a343-49ab-4d2a-bcd6-acb1fd914896"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 定义参数网格\n",
        "param_grid = {\n",
        "    'units1': [64, 128],\n",
        "    'batch_size': [64, 128],\n",
        "    'epochs': [100, 150],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "# 准备存储最优模型及其性能\n",
        "best_score = -1\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# 手动遍历所有参数组合\n",
        "param_combinations = list(itertools.product(param_grid['units1'],\n",
        "                                            param_grid['batch_size'],\n",
        "                                            param_grid['epochs'],\n",
        "                                            param_grid['optimizer']))\n",
        "\n",
        "for combination in param_combinations:\n",
        "    units1, batch_size, epochs, optimizer = combination\n",
        "    units2 = units1 // 2  # 确保 units2 是 units1 的一半\n",
        "\n",
        "    # 构建模型\n",
        "    model = build_lstm_model()  # 使用之前定义的 build_lstm_model 函数\n",
        "\n",
        "    # 编译模型\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # 创建早停回调\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # 训练模型\n",
        "    history = model.fit([X_seq_train_resampled, X_static_train_resampled], y_train_resampled,\n",
        "                        batch_size=batch_size, epochs=epochs,\n",
        "                        validation_split=0.2, callbacks=[early_stopping],\n",
        "                        class_weight=class_weights_dict, verbose=2)\n",
        "\n",
        "    # 在验证集上评估模型\n",
        "    val_loss, val_acc = model.evaluate([X_seq_val_resampled, X_static_val_resampled], y_val_resampled, verbose=2)\n",
        "\n",
        "    # 如果当前参数组合表现更好，则更新最佳参数和模型\n",
        "    if val_acc > best_score:\n",
        "        best_score = val_acc\n",
        "        best_params = {\n",
        "            'units1': units1,\n",
        "            'units2': units2,\n",
        "            'batch_size': batch_size,\n",
        "            'epochs': epochs,\n",
        "            'optimizer': optimizer\n",
        "        }\n",
        "        best_model = model\n",
        "\n",
        "# 输出最佳参数组合和对应的性能\n",
        "print(f\"Best score: {best_score} with parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2esRVxCLHgMB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE8p_BxL-ykn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA4sU0XOOPvb"
      },
      "outputs": [],
      "source": [
        "# 实例化和编译模型\n",
        "model = build_lstm_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 创建 EarlyStopping 回调实例\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # 监控验证损失\n",
        "    patience=5,          # 如果验证损失在 5 个 epoch 内没有改善，则停止训练\n",
        "    verbose=1,           # 显示日志信息\n",
        "    mode='min',          # 验证损失要最小化\n",
        "    restore_best_weights=True  # 恢复模型到验证集表现最好的权重\n",
        ")\n",
        "\n",
        "# 训练模型\n",
        "model.fit(\n",
        "    [X_seq_train_resampled, X_static_train[:X_seq_train_resampled.shape[0]]], y_train_resampled,\n",
        "    epochs=80,\n",
        "    batch_size=32,\n",
        "    validation_data=([X_seq_test, X_static_test[:X_seq_test.shape[0]]], y_test),\n",
        "    class_weight=class_weights_dict,  # 应用类别权重\n",
        "    callbacks=[early_stopping],  # 添加 EarlyStopping 回调\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 评估模型\n",
        "loss, accuracy = model.evaluate([X_seq_test, X_static_test], y_test)\n",
        "print(f\"测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# 预测并输出性能指标\n",
        "y_pred = model.predict([X_seq_test, X_static_test])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\n混淆矩阵:\")\n",
        "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
        "\n",
        "print(\"\\n分类报告:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7MXt2tmPwwS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, Attention\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 假设df_imputed是已经填补好缺失值的数据集\n",
        "\n",
        "# 选择需要标准化的特征\n",
        "features_to_scale = ['Bicarbonate', 'Creatinine', 'Glucose', 'Oxygen Saturation', 'Platelet Count', 'Potassium']\n",
        "\n",
        "# 标准化数据\n",
        "scaler = StandardScaler()\n",
        "df_imputed[features_to_scale] = scaler.fit_transform(df_imputed[features_to_scale])\n",
        "\n",
        "# 构建 LSTM 时间序列数据\n",
        "def create_sequences(data, time_steps=22):\n",
        "    X, X_static, y = [], [], []\n",
        "    for _, group in data.groupby('subject_id'):\n",
        "        feature_data = group[features_to_scale].values\n",
        "        static_data = group[['gender', 'aki_age', 'bmi']].values\n",
        "        target_data = group['survival_category'].values\n",
        "\n",
        "        if len(feature_data) >= time_steps:\n",
        "            for i in range(len(feature_data) - time_steps + 1):\n",
        "                X.append(feature_data[i:i+time_steps])\n",
        "                X_static.append(static_data[i+time_steps-1])  # 取最后一个时间步的静态特征\n",
        "                y.append(target_data[i+time_steps-1])\n",
        "    return np.array(X), np.array(X_static), np.array(y)\n",
        "\n",
        "# 生成数据\n",
        "time_steps = 22\n",
        "X_sequence, X_static, y = create_sequences(df_imputed, time_steps)\n",
        "\n",
        "# 转换目标标记为 one-hot 编码\n",
        "y_categorical = to_categorical(y, num_classes=4)\n",
        "\n",
        "# 分割数据集\n",
        "X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(\n",
        "    X_sequence, X_static, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# 确保 X_train 和 y_train 是浮点型\n",
        "X_seq_train = np.array(X_seq_train, dtype=np.float32)\n",
        "X_seq_test = np.array(X_seq_test, dtype=np.float32)\n",
        "X_static_train = np.array(X_static_train, dtype=np.float32)\n",
        "X_static_test = np.array(X_static_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoKEewV0P153",
        "outputId": "8af1edae-a663-49fe-a690-336687b62e10"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already defined X_seq_train and y_train\n",
        "n_samples, time_steps, n_features = X_seq_train.shape\n",
        "\n",
        "# Flatten the sequence data\n",
        "X_seq_train_flat = X_seq_train.reshape(n_samples, -1)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_seq_train_resampled_flat, y_train_resampled = smote.fit_resample(X_seq_train_flat, y_train)\n",
        "\n",
        "# Reshape back to the original sequence format\n",
        "X_seq_train_resampled = X_seq_train_resampled_flat.reshape(-1, time_steps, n_features)\n",
        "\n",
        "# Adjust X_static_train to match the resampled sequence data\n",
        "num_resampled_samples = len(X_seq_train_resampled)\n",
        "num_static_samples = len(X_static_train)\n",
        "\n",
        "# Calculate the required number of repetitions\n",
        "num_repeats = num_resampled_samples // num_static_samples\n",
        "remainder = num_resampled_samples % num_static_samples\n",
        "\n",
        "# Repeat and concatenate the static data to match the sequence data samples\n",
        "X_static_train_resampled = np.vstack((np.tile(X_static_train, (num_repeats, 1)),\n",
        "                                      X_static_train[:remainder]))\n",
        "\n",
        "print(X_static_train_resampled.shape)\n",
        "print(X_seq_train_resampled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkE6nlWtQ9qn",
        "outputId": "d18b0143-c1f9-4e28-b605-0d100f98aa22"
      },
      "outputs": [],
      "source": [
        "print(X_static_train_resampled.shape)\n",
        "print(X_seq_train_resampled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsY2mZjRQR5a"
      },
      "outputs": [],
      "source": [
        "# 构建 LSTM 模型\n",
        "def build_lstm_model():\n",
        "    sequence_input = Input(shape=(X_seq_train_resampled.shape[1], X_seq_train_resampled.shape[2]), name='sequence_input')\n",
        "    static_input = Input(shape=(X_static_train_resampled.shape[1],), name='static_input')\n",
        "\n",
        "    # LSTM 层\n",
        "    x = LSTM(64, return_sequences=True)(sequence_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # 自注意力层\n",
        "    attn_layer = Attention()([x, x])\n",
        "\n",
        "    x = LSTM(32)(attn_layer)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # 将静态特征与 LSTM 输出连接\n",
        "    x = Concatenate()([x, static_input])\n",
        "\n",
        "    # 全连接层\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    # 构建模型\n",
        "    model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# 实例化和编译模型\n",
        "model = build_lstm_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddMARSuqQS-L",
        "outputId": "9bfb1760-45f8-4afb-fa60-d291db76b900"
      },
      "outputs": [],
      "source": [
        "# 创建 EarlyStopping 回调实例\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # 监控验证损失\n",
        "    patience=5,          # 如果验证损失在 5 个 epoch 内没有改善，则停止训练\n",
        "    verbose=1,           # 显示日志信息\n",
        "    mode='min',          # 验证损失要最小化\n",
        "    restore_best_weights=True  # 恢复模型到验证集表现最好的权重\n",
        ")\n",
        "\n",
        "# 训练模型\n",
        "model_details=model.fit(\n",
        "    [X_seq_train_resampled, X_static_train_resampled], y_train_resampled,\n",
        "    epochs=80,\n",
        "    batch_size=64,\n",
        "    validation_data=([X_seq_test, X_static_test], y_test),\n",
        "    callbacks=[early_stopping],  # 添加 EarlyStopping 回调\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P7KxqzMR6qs",
        "outputId": "ca05d62b-0308-49f5-dcc8-d565d4f88b00"
      },
      "outputs": [],
      "source": [
        "# 评估模型\n",
        "loss, accuracy = model.evaluate([X_seq_test, X_static_test], y_test)\n",
        "print(f\"测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# 预测并输出性能指标\n",
        "y_pred = model.predict([X_seq_test, X_static_test])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\n混淆矩阵:\")\n",
        "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
        "\n",
        "print(\"\\n分类报告:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "m-SHExGSs_2C",
        "outputId": "d2f2ad23-8a7d-4b67-dbcd-25091e53d65a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 绘制训练损失和验证损失的图表\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(model_details.history['loss'], label='Training Loss')\n",
        "plt.plot(model_details.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HsoMaY80678"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "RJVXL6d0LFHi",
        "outputId": "b5bbbb32-f508-49cc-8703-598c31cb4b95"
      },
      "outputs": [],
      "source": [
        "print(X_seq_train.shape)\n",
        "print(X_static_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwEXjhnFP6BU",
        "outputId": "172dbb58-3a97-4630-df3a-b0235edf5b4b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmepyfRkQRXY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZFy1BsLwkEO"
      },
      "source": [
        "# 下面的是之前的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ3WHy4fz-oE"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3jVp_xpzXAL"
      },
      "outputs": [],
      "source": [
        "# # 选择需要标准化的特征\n",
        "# features_to_scale = ['Bicarbonate', 'Creatinine', 'Glucose', 'Oxygen Saturation', 'Platelet Count', 'Potassium']\n",
        "\n",
        "# # 标准化数据\n",
        "# scaler = StandardScaler()\n",
        "# df_all[features_to_scale] = scaler.fit_transform(df_all[features_to_scale])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCSqB3Zf0FeL"
      },
      "outputs": [],
      "source": [
        "# # 构建 LSTM 时间序列数据\n",
        "# def create_sequences(data, time_steps=22):\n",
        "#     X, y = [], []\n",
        "#     for _, group in data.groupby('subject_id'):\n",
        "#         feature_data = group[features_to_scale + ['gender', 'aki_age', 'bmi']].values\n",
        "#         target_data = group['survival_category'].values\n",
        "#         if len(feature_data) >= time_steps:\n",
        "#             for i in range(len(feature_data) - time_steps + 1):\n",
        "#                 X.append(feature_data[i:i+time_steps])\n",
        "#                 y.append(target_data[i+time_steps-1])\n",
        "#     return np.array(X), np.array(y)\n",
        "\n",
        "# # 生成数据\n",
        "# time_steps = 22\n",
        "# X, y = create_sequences(df_all, time_steps)\n",
        "\n",
        "# # 转换目标变量为 one-hot 编码\n",
        "# y_categorical = to_categorical(y, num_classes=4)\n",
        "\n",
        "# # 分割数据集\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
        "# 构建 LSTM 时间序列数据\n",
        "def create_sequences(data, time_steps=22):\n",
        "    X, X_static, y = [], [], []\n",
        "    for _, group in data.groupby('subject_id'):\n",
        "        feature_data = group[features_to_scale].values\n",
        "        static_data = group[['gender', 'aki_age', 'bmi']].values\n",
        "        target_data = group['survival_category'].values\n",
        "\n",
        "        if len(feature_data) >= time_steps:\n",
        "            for i in range(len(feature_data) - time_steps + 1):\n",
        "                X.append(feature_data[i:i+time_steps])\n",
        "                X_static.append(static_data[i+time_steps-1])  # 取最后一个时间步的静态特征\n",
        "                y.append(target_data[i+time_steps-1])\n",
        "    return np.array(X), np.array(X_static), np.array(y)\n",
        "\n",
        "# 生成数据\n",
        "time_steps = 22\n",
        "X_sequence, X_static, y = create_sequences(df_all, time_steps)\n",
        "\n",
        "# 转换目标标记为 one-hot 编码\n",
        "y_categorical = to_categorical(y, num_classes=4)\n",
        "\n",
        "# 分割数据集\n",
        "X_seq_train, X_seq_test, X_static_train, X_static_test, y_train, y_test = train_test_split(\n",
        "    X_sequence, X_static, y_categorical, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGoxnsfH0wNW"
      },
      "outputs": [],
      "source": [
        "# 确保 X_train 和 y_train 是浮点型\n",
        "X_seq_train = np.array(X_seq_train, dtype=np.float32)\n",
        "X_seq_test = np.array(X_seq_test, dtype=np.float32)\n",
        "X_static_train = np.array(X_static_train, dtype=np.float32)\n",
        "X_static_test = np.array(X_static_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "#import numpy as np\n",
        "#X_train 和 y_train 是浮点型\n",
        "# X_train = np.array(X_train, dtype=np.float32)\n",
        "# X_test = np.array(X_test, dtype=np.float32)\n",
        "\n",
        "# # y_train 和 y_test 已经是 one-hot 编码，确保为浮点型\n",
        "# y_train = np.array(y_train, dtype=np.float32)\n",
        "# y_test = np.array(y_test, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl-Ju0cY0NcK",
        "outputId": "8b4b6131-a8a9-4058-9a8b-d39e2707cd04"
      },
      "outputs": [],
      "source": [
        "# # 构建 LSTM 模型\n",
        "# model = Sequential([\n",
        "#     LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
        "#     Dropout(0.2),\n",
        "#     LSTM(32),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(4, activation='softmax')  # 输出4个类别的概率\n",
        "# ])\n",
        "\n",
        "# 构建 LSTM 模型\n",
        "def build_lstm_model():\n",
        "    sequence_input = Input(shape=(X_seq_train.shape[1], X_seq_train.shape[2]), name='sequence_input')\n",
        "    static_input = Input(shape=(X_static_train.shape[1],), name='static_input')\n",
        "\n",
        "    # LSTM 层\n",
        "    x = LSTM(64, return_sequences=True)(sequence_input)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = LSTM(32)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # 将静态特征与 LSTM 输出连接\n",
        "    x = Concatenate()([x, static_input])\n",
        "\n",
        "    # 全连接层\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    # 构建模型\n",
        "    model = Model(inputs=[sequence_input, static_input], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 实例化和编译模型\n",
        "model = build_lstm_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 创建 EarlyStopping 回调实例\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # 监控验证损失\n",
        "    patience=5,          # 如果验证损失在 5 个 epoch 内没有改善，则停止训练\n",
        "    verbose=1,           # 显示日志信息\n",
        "    mode='min',          # 验证损失要最小化\n",
        "    restore_best_weights=True  # 恢复模型到验证集表现最好的权重\n",
        ")\n",
        "\n",
        "# 训练模型\n",
        "model.fit(\n",
        "    [X_seq_train, X_static_train], y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=([X_seq_test, X_static_test], y_test),\n",
        "    callbacks=[early_stopping],  # 添加 EarlyStopping 回调\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# model = build_lstm_model()\n",
        "\n",
        "# # 编译模型\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # 创建 EarlyStopping 回调实例\n",
        "# early_stopping = EarlyStopping(\n",
        "#     monitor='val_loss',  # 监控验证损失\n",
        "#     patience=5,          # 如果验证损失在 5 个 epoch 内没有改善，则停止训练\n",
        "#     verbose=1,           # 显示日志信息\n",
        "#     mode='min',          # 验证损失要最小化\n",
        "#     restore_best_weights=True  # 恢复模型到验证集表现最好的权重\n",
        "# )\n",
        "\n",
        "# # 训练模型\n",
        "# model.fit(X_train, y_train,\n",
        "#           epochs=50,\n",
        "#           batch_size=32,\n",
        "#           validation_data=(X_test, y_test),\n",
        "#           callbacks=[early_stopping],  # 添加 EarlyStopping 回调\n",
        "#           verbose=1)\n",
        "\n",
        "# # 评估模型\n",
        "# loss, accuracy = model.evaluate(X_test, y_test)\n",
        "# print(f\"测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# # 预测并输出性能指标\n",
        "# y_pred = model.predict(X_test)\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "# y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# print(\"\\n混淆矩阵:\")\n",
        "# print(confusion_matrix(y_true_classes, y_pred_classes))\n",
        "\n",
        "# print(\"\\n分类报告:\")\n",
        "# print(classification_report(y_true_classes, y_pred_classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKObN6gO6PNC",
        "outputId": "8c9f2103-1f8d-47ef-cd8a-a0b001c5e8fa"
      },
      "outputs": [],
      "source": [
        "# 评估模型\n",
        "loss, accuracy = model.evaluate([X_seq_test, X_static_test], y_test)\n",
        "print(f\"测试集准确率: {accuracy:.4f}\")\n",
        "\n",
        "# 预测并输出性能指标\n",
        "y_pred = model.predict([X_seq_test, X_static_test])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\n混淆矩阵:\")\n",
        "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
        "\n",
        "print(\"\\n分类报告:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQpyfAHWcP1O"
      },
      "source": [
        "# 后面是第二种缺失值处理的方法，区别在于先删除50%以上缺失数据的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "tyFBVk6Rccir",
        "outputId": "cf90f7f5-35bc-464f-8308-5b18c7c99042"
      },
      "outputs": [],
      "source": [
        "df_all1 = df_pivot.copy()\n",
        "\n",
        "df_all1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "04DBcA0xccwz",
        "outputId": "5d4d0c5a-3d00-4c23-dd5e-370f5fa34f26"
      },
      "outputs": [],
      "source": [
        "# Set the threshold for missing values\n",
        "\n",
        "threshold = 0.5  # Set the threshold for missing values\n",
        "\n",
        "df_all1 = df_all1.dropna(thresh=int(threshold * df_all1.shape[1]))\n",
        "\n",
        "df_all1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_kAdZUuccz_",
        "outputId": "a68edb53-4b3f-427a-f6f4-c48a580bfba9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S2NvxowdSZX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
