{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import packages\n",
    "\n",
    "# Database and cloud storage\n",
    "import psycopg2\n",
    "import gcsfs\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "\n",
    "# Asynchronous operations\n",
    "import aiohttp\n",
    "import ssl\n",
    "import certifi\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openpyxl\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, cross_val_predict\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Deep learning with TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged CSV file\n",
    "merged_file_path = '/Users/tyh/Desktop/Coding/thesis/data/thesis_aki_patients_classci.csv'\n",
    "df = pd.read_csv(merged_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# survival risk category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_survival_days(days):\n",
    "    if days <= 200:\n",
    "        return '0'  #Short-term survival\n",
    "    elif 200 < days <= 800:\n",
    "        return '1'  #Medium-term survival\n",
    "    elif 800 < days <= 1500:\n",
    "        return '2'  #Long-term survival\n",
    "    else:\n",
    "        return '3'  #Very long-term survival\n",
    "    \n",
    "# the categorize of  survival_days \n",
    "df['survival_category'] = df['survival_days'].apply(categorize_survival_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of each category in survival_category\n",
    "compute_df_category_counts = df['survival_category'].value_counts()\n",
    "\n",
    "# Calculation ratio\n",
    "compute_df_total_samples = len(df)\n",
    "compute_df_category_proportions = compute_df_category_counts / compute_df_total_samples\n",
    "\n",
    "print(compute_df_category_counts)\n",
    "print(compute_df_category_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop the columns that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extracted = df.drop(columns=['confirm_aki_time', 'final_deathtime'])\n",
    "df_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "missing_values = df_extracted.isnull().sum()\n",
    "print(\"the num of missing_values:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prcessing the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly， Remove columns with all values as missing values, then to ensure data robustness, patients are deleted if more than 50 per cent of their data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Remove all columns with missing values\n",
    "df_cleaned = df_extracted.dropna(axis=1, how='all')\n",
    "\n",
    "# print the shape of the cleaned DataFrame\n",
    "print(\"Shape after removal of all empty columns:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondly, set threshold = 0.5 for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for missing values\n",
    "threshold = 0.5  # Set the threshold for missing values\n",
    "df_cleaned_new = df_cleaned.dropna(thresh=int(threshold * df_cleaned.shape[1]))\n",
    "\n",
    "df_cleaned_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completing missing values，using MICE to fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the value column\n",
    "numeric_columns = df_cleaned_new.select_dtypes(include=[float, int]).columns\n",
    "# Using MICE to Fill in Missing Values in Value Columns\n",
    "mice_imputer = IterativeImputer(max_iter=10, random_state=42, imputation_order='ascending', initial_strategy='mean')\n",
    "\n",
    "df_numeric_imputed = pd.DataFrame(mice_imputer.fit_transform(df_cleaned_new[numeric_columns]), columns=numeric_columns)\n",
    "\n",
    "df_numeric_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are still missing values in the filled DataFrame\n",
    "final_missing_values = df_numeric_imputed.isnull().sum()\n",
    "print(\"Missing value statistics after filling:\")\n",
    "print(final_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_imputed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defination the categorize of survival_days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_survival_days(days):\n",
    "    if days <= 200:\n",
    "        return '0'  #Short-term survival\n",
    "    elif 200 < days <= 800:\n",
    "        return '1'  #Medium-term survival\n",
    "    elif 800 < days <= 1500:\n",
    "        return '2'  #Long-term survival\n",
    "    else:\n",
    "        return '3'  #Very long-term survival\n",
    "    \n",
    "# the categorize of  survival_days \n",
    "df_numeric_imputed['survival_category'] = df_numeric_imputed['survival_days'].apply(categorize_survival_days)\n",
    "\n",
    "df_numeric_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of each category in survival_category\n",
    "category_counts = df_numeric_imputed['survival_category'].value_counts()\n",
    "\n",
    "# Calculation ratio\n",
    "total_samples = len(df)\n",
    "category_proportions = category_counts / total_samples\n",
    "\n",
    "print(category_counts)\n",
    "print(category_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation, min - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min\n",
    "columns_to_normalize = [\n",
    "    'bmi', \n",
    "    'after_aki_creatinine_max', 'after_aki_creatinine_min',\n",
    "    'after_aki_glucose_max', 'after_aki_glucose_min',\n",
    "    'after_aki_potassium_max', 'after_aki_potassium_min',\n",
    "    'after_aki_platelet_max', 'after_aki_platelet_min',\n",
    "    'after_aki_sp_o2_max', 'after_aki_sp_o2_min',\n",
    "    'creatinine_max', 'creatinine_min',\n",
    "    'glucose_max', 'glucose_min',\n",
    "    'potassium_max', 'potassium_min',\n",
    "    'platelet_max', 'platelet_min',\n",
    "    'sp_o2_max', 'sp_o2_min'\n",
    "]\n",
    "\n",
    "# 初始化 MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 对指定列进行归一化，结果存储在一个新的 DataFrame 中\n",
    "df_final = df_numeric_imputed.copy()\n",
    "df_final[columns_to_normalize] = scaler.fit_transform(df_numeric_imputed[columns_to_normalize])\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imbalance exists, oversampling using SMOTE, and split the dataset into training and test sets, finally, perform feature standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of features and target variables\n",
    "features = [\n",
    "    'gender','bmi','aki_age', \n",
    "    'after_aki_creatinine_max', 'after_aki_creatinine_min',\n",
    "    'after_aki_glucose_max', 'after_aki_glucose_min',\n",
    "    'after_aki_potassium_max', 'after_aki_potassium_min',\n",
    "    'after_aki_platelet_max', 'after_aki_platelet_min',\n",
    "    'after_aki_sp_o2_max', 'after_aki_sp_o2_min',\n",
    "    'creatinine_max', 'creatinine_min',\n",
    "    'glucose_max', 'glucose_min',\n",
    "    'potassium_max', 'potassium_min',\n",
    "    'platelet_max', 'platelet_min',\n",
    "    'sp_o2_max', 'sp_o2_min'\n",
    "    ]\n",
    "    \n",
    "X = df_final[features]\n",
    "y = df_final['survival_category']\n",
    "\n",
    "# Encoding the target variable as a number\n",
    "y = y.astype('category').cat.codes\n",
    "\n",
    "# oversampling using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature standardisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Model Part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model and train it\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on test sets\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "precision = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross validation and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for the logistic regression model\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # 正则化参数\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],  # 求解器\n",
    "    'max_iter': [100, 200, 300]  # 最大迭代次数\n",
    "}\n",
    "\n",
    "# Creating a logistic regressionl model\n",
    "lr = LogisticRegression(multi_class='multinomial', random_state=42)\n",
    "\n",
    "# Setting Grid Search Parameters\n",
    "lr_grid_search = GridSearchCV(\n",
    "    estimator=lr,\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=10,  # 10折交叉验证\n",
    "    scoring='accuracy',  # 使用准确率作为评价指标\n",
    "    n_jobs=-1,  # 使用所有可用的CPU核心\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "lr_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print the best parameters and best model score\n",
    "print(\"Best parameters found: \", lr_grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", lr_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the best parameters to repeat prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model to make predictions\n",
    "lr_best_model = lr_grid_search.best_estimator_\n",
    "y_pred_lr = lr_best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "precision = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"\\nModel Performance Metrics on Test Data:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of LR model hyperparametric combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openpyxl\n",
    "\n",
    "# the results of the grid search\n",
    "lr_results = pd.DataFrame(lr_grid_search.cv_results_)\n",
    "\n",
    "# the accuray of cross validation select hyperparametric combinations \n",
    "lr_results_acc = lr_results[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "lr_results_acc\n",
    "\n",
    "# save the results to an Excel file to local\n",
    "lr_results_acc.to_excel('lr_grid_search_results.xlsx', index=False)\n",
    "\n",
    "print(\"Results saved to 'lr_grid_search_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200, random_state=42)\n",
    "\n",
    "# 10-fold cross-validation to compute the accuracy of the model\n",
    "cv_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Calculate the average accuracy\n",
    "mean_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the accuracy of each fold and the average accuracy\n",
    "print(f\"Per-fold accuracy for 10-fold cross validation: {cv_scores}\")\n",
    "print(f\"Average accuracy of 10-fold cross-validation: {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR - AUC-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# 4 classes\n",
    "n_classes = 4\n",
    "y_train_binarized = label_binarize(y_train, classes=[0, 1, 2, 3])\n",
    "\n",
    "# 10-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store the micro-average and weighted-average AUC values\n",
    "micro_aucs = []\n",
    "weighted_aucs = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_train_scaled, y_train):\n",
    "    # Indexing with NumPy arrays to avoid Pandas indexing errors\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_binarized[train_idx], y_train_binarized[val_idx]\n",
    "    \n",
    "    lr_model.fit(X_train_fold, y_train[train_idx])\n",
    "    y_pred_fold = lr_model.predict_proba(X_val_fold)\n",
    "    \n",
    "    micro_auc_fold = roc_auc_score(y_val_fold, y_pred_fold, average='micro')\n",
    "    weighted_auc_fold = roc_auc_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "    \n",
    "    micro_aucs.append(micro_auc_fold)\n",
    "    weighted_aucs.append(weighted_auc_fold)\n",
    "\n",
    "# Calculate the mean and standard deviation of the AUC values\n",
    "micro_auc_cv_mean = np.mean(micro_aucs)\n",
    "micro_auc_cv_std = np.std(micro_aucs)\n",
    "weighted_auc_cv_mean = np.mean(weighted_aucs)\n",
    "weighted_auc_cv_std = np.std(weighted_aucs)\n",
    "\n",
    "print(f\"10-Fold Cross-Validation Micro-Average AUC: Mean = {micro_auc_cv_mean:.4f}, Std Dev = {micro_auc_cv_std:.4f}\")\n",
    "print(f\"10-Fold Cross-Validation Weighted-Average AUC: Mean = {weighted_auc_cv_mean:.4f}, Std Dev = {weighted_auc_cv_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the FPR and TPR values for each fold\n",
    "fprs_micro = []\n",
    "tprs_micro = []\n",
    "fprs_weighted = []\n",
    "tprs_weighted = []\n",
    "\n",
    "# plt roc curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_train_scaled, y_train):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_binarized[train_idx], y_train_binarized[val_idx]\n",
    "    \n",
    "    lr_model.fit(X_train_fold, y_train[train_idx])\n",
    "    y_pred_fold = lr_model.predict_proba(X_val_fold)\n",
    "    \n",
    "    # compute Micro-Average ROC curve\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_val_fold.ravel(), y_pred_fold.ravel())\n",
    "    fprs_micro.append(fpr_micro)\n",
    "    tprs_micro.append(np.interp(np.linspace(0, 1, 100), fpr_micro, tpr_micro))\n",
    "    \n",
    "    # compute Weighted-Average ROC curve\n",
    "    fpr_weighted, tpr_weighted, _ = roc_curve(y_val_fold.ravel(), y_pred_fold.ravel())\n",
    "    fprs_weighted.append(fpr_weighted)\n",
    "    tprs_weighted.append(np.interp(np.linspace(0, 1, 100), fpr_weighted, tpr_weighted))\n",
    "\n",
    "# Micro-Average ROC curve\n",
    "mean_fpr_micro = np.linspace(0, 1, 100)\n",
    "mean_tpr_micro = np.mean(tprs_micro, axis=0)\n",
    "mean_tpr_micro[-1] = 1.0\n",
    "micro_auc = auc(mean_fpr_micro, mean_tpr_micro)\n",
    "\n",
    "plt.plot(mean_fpr_micro, mean_tpr_micro, color='blue', lw=4,  # \n",
    "         label=f'Micro-Average ROC (AUC = {micro_auc_cv_mean:.4f} ± {micro_auc_cv_std:.4f})')\n",
    "\n",
    "# Weighted-Average ROC curve\n",
    "mean_fpr_weighted = np.linspace(0, 1, 100)\n",
    "mean_tpr_weighted = np.mean(tprs_weighted, axis=0)\n",
    "mean_tpr_weighted[-1] = 1.0\n",
    "weighted_auc = auc(mean_fpr_weighted, mean_tpr_weighted)\n",
    "\n",
    "plt.plot(mean_fpr_weighted, mean_tpr_weighted, color='red', lw=2, \n",
    "         label=f'Weighted-Average ROC (AUC = {weighted_auc_cv_mean:.4f} ± {weighted_auc_cv_std:.4f})')\n",
    "\n",
    "# 45-degree line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey', lw=2)\n",
    "\n",
    "# drop the grid\n",
    "plt.grid(False)\n",
    "\n",
    "# set the x and y axis limits\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=15)\n",
    "plt.ylabel('True Positive Rate', fontsize=15)\n",
    "plt.title('ROC Curve for 10-Fold Cross-Validation', fontsize=18)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "# print plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the best parameters AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the test labels for AUC calculation\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "\n",
    "# Fit the best model on the training data\n",
    "y_pred_best = lr_best_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Compute the Micro-Average AUC and Weighted-Average AUC\n",
    "n_bootstrap = 1000\n",
    "micro_aucs_bootstrap = []\n",
    "weighted_aucs_bootstrap = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(np.arange(len(y_test)), size=len(y_test), replace=True)\n",
    "    y_test_sample = y_test_binarized[indices]\n",
    "    y_pred_sample = y_pred_best[indices]\n",
    "    \n",
    "    micro_auc_bootstrap = roc_auc_score(y_test_sample, y_pred_sample, average='micro')\n",
    "    weighted_auc_bootstrap = roc_auc_score(y_test_sample, y_pred_sample, average='weighted')\n",
    "    \n",
    "    micro_aucs_bootstrap.append(micro_auc_bootstrap)\n",
    "    weighted_aucs_bootstrap.append(weighted_auc_bootstrap)\n",
    "\n",
    "# Calculate the mean and standard deviation of the AUC values\n",
    "micro_auc_best_mean = np.mean(micro_aucs_bootstrap)\n",
    "micro_auc_best_std = np.std(micro_aucs_bootstrap)\n",
    "weighted_auc_best_mean = np.mean(weighted_aucs_bootstrap)\n",
    "weighted_auc_best_std = np.std(weighted_aucs_bootstrap)\n",
    "\n",
    "print(f\"Best Model Micro-Average AUC: Mean = {micro_auc_best_mean:.4f}, Std Dev = {micro_auc_best_std:.4f}\")\n",
    "print(f\"Best Model Weighted-Average AUC: Mean = {weighted_auc_best_mean:.4f}, Std Dev = {weighted_auc_best_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the test labels for AUC calculation\n",
    "y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "\n",
    "# Fit the best model on the training data\n",
    "y_pred_best = lr_best_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# compute Micro-Average ROC curve\n",
    "fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), y_pred_best.ravel())\n",
    "micro_auc = auc(fpr_micro, tpr_micro)\n",
    "\n",
    "# compute Weighted-Average ROC curve\n",
    "fpr_weighted, tpr_weighted, _ = roc_curve(y_test_binarized.ravel(), y_pred_best.ravel())\n",
    "weighted_auc = auc(fpr_weighted, tpr_weighted)\n",
    "\n",
    "# plt roc curve \n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Micro-Average ROC curve\n",
    "plt.plot(fpr_micro, tpr_micro, color='blue', lw=5, \n",
    "         label=f'Micro-Average ROC (AUC = {micro_auc_best_mean:.4f} ± {micro_auc_best_std:.4f})')\n",
    "\n",
    "# Weighted-Average ROC curve\n",
    "plt.plot(fpr_weighted, tpr_weighted, color='red', lw=2, \n",
    "         label=f'Weighted-Average ROC (AUC = {weighted_auc_best_mean:.4f} ± {weighted_auc_best_std:.4f})')\n",
    "\n",
    "# 45-degree line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey', lw=2)\n",
    "\n",
    "# drop the grid\n",
    "plt.grid(False)\n",
    "\n",
    "# set the x and y axis limits\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=15)\n",
    "plt.ylabel('True Positive Rate', fontsize=15)\n",
    "plt.title('ROC Curve for Best Model on Test Data', fontsize=18)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "# plt \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create svm model and fit it\n",
    "svm_classifier = SVC(kernel='rbf', decision_function_shape='ovo', random_state=42)\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on test sets\n",
    "y_pred_svm = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "precision = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"SVM Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nSVM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],        # \n",
    "    'gamma': [0.01, 0.1, 1, 10],   # \n",
    "    'kernel': ['linear', 'rbf'],    # \n",
    "    'decision_function_shape':['ovo','ovr']\n",
    "}\n",
    "\n",
    "# create a SVM model\n",
    "svm_model = SVC(decision_function_shape='ovr', random_state=42)\n",
    "\n",
    "# Setting Grid Search Parameters\n",
    "svm_grid_search = GridSearchCV(\n",
    "    estimator=svm_model,\n",
    "    param_grid=svm_param_grid,\n",
    "    cv=10,  # \n",
    "    scoring='accuracy',  # \n",
    "    n_jobs=-1,  #\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "svm_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print the best parameters and best model score\n",
    "print(\"Best parameters found: \", svm_grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", svm_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model to make predictions\n",
    "best_svm_model = svm_grid_search.best_estimator_\n",
    "y_pred_svm = best_svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "precision = precision_score(y_test, y_pred_svm, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_svm, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"SVM Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of svm model hyperparametric combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results = pd.DataFrame(svm_grid_search.cv_results_)\n",
    "\n",
    "svm_results_acc = svm_results[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "svm_results_acc\n",
    "\n",
    "svm_results_acc.to_excel('svm_grid_search_results.xlsx', index=False)\n",
    "\n",
    "print(\"Results saved to 'svm_grid_search_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_classifier = SVC(kernel='rbf', decision_function_shape='ovo', random_state=42)\n",
    "svm_classifier = SVC(decision_function_shape='ovr', random_state=42, probability=True)\n",
    "\n",
    "# 10-fold cross-validation to compute the accuracy of the model\n",
    "cv_scores = cross_val_score(svm_classifier, X_train_scaled, y_train, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Calculate the average accuracy\n",
    "mean_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the accuracy of each fold and the average accuracy\n",
    "print(f\"Per-fold accuracy for 10-fold cross validation: {cv_scores}\")\n",
    "print(f\"Average accuracy of 10-fold cross-validation: {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 4\n",
    "classes = [0, 1, 2, 3]\n",
    "\n",
    "svm_classifier = SVC(kernel='rbf', decision_function_shape='ovo', random_state=42, probability=True)\n",
    "\n",
    "# Binarize the labels for AUC calculation\n",
    "y_train_bin = label_binarize(y_train, classes=classes)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "# 10-fold cross-validation\n",
    "# svm_classifier = SVC(kernel='rbf', decision_function_shape='ovo', probability=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store the micro-average and weighted-average AUC values\n",
    "y_scores_cv = cross_val_predict(svm_classifier, X_train_scaled, y_train, cv=3, method=\"predict_proba\", n_jobs=-1)\n",
    "\n",
    "# compute Micro-Average AUC\n",
    "micro_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"micro\")\n",
    "micro_auc_cv_std = np.std(cross_val_score(svm_classifier, X_train_scaled, y_train, cv=3, scoring='roc_auc_ovr'))\n",
    "\n",
    "# compute Weighted-Average AUC\n",
    "weighted_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"weighted\")\n",
    "weighted_auc_cv_std = np.std(cross_val_score(svm_classifier, X_train_scaled, y_train, cv=3, scoring='roc_auc_ovr'))\n",
    "\n",
    "print(f\"10 fold cross validation Micro-Average AUC: Mean = {micro_auc_cv:.4f}, Std Dev = {micro_auc_cv_std:.4f}\")\n",
    "print(f\"10 fold cross validation Weighted-Average AUC: Mean = {weighted_auc_cv:.4f}, Std Dev = {weighted_auc_cv_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_model= SVC(C=100,kernel='rbf', decision_function_shape='ovo', gamma=0.1, random_state=42, probability=True)\n",
    "\n",
    "# 在整个训练集上拟合最佳模型\n",
    "best_svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Initialize lists to store the FPR and TPR values for each fold\n",
    "y_scores_best = best_svm_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# compute Micro-Average ROC curve\n",
    "micro_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"micro\")\n",
    "micro_auc_best_std = np.std(cross_val_score(best_svm_model, X_train_scaled, y_train, cv=3, scoring='roc_auc_ovr'))\n",
    "\n",
    "# compute Weighted-Average ROC curve\n",
    "weighted_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"weighted\")\n",
    "weighted_auc_best_std = np.std(cross_val_score(best_svm_model, X_train_scaled, y_train, cv=3, scoring='roc_auc_ovr'))\n",
    "\n",
    "# print the AUC values\n",
    "print(f\"The best Micro-Average AUC: Mean = {micro_auc_best:.4f}, Std Dev = {micro_auc_best_std:.4f}\")\n",
    "print(f\"The best Weighted-Average AUC: Mean = {weighted_auc_best:.4f}, Std Dev = {weighted_auc_best_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "micro_auc_cv=0.9238\n",
    "micro_auc_cv_std=0.0030\n",
    "\n",
    "\n",
    "weighted_auc_cv=0.9112\n",
    "weighted_auc_cv_std=0.0030\n",
    "\n",
    "# 绘制交叉验证的 AUC 和方差的 ROC 曲线\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "\n",
    "# Micro-Average ROC Curve for cross-validation\n",
    "plt.plot(fpr_micro_cv, tpr_micro_cv, color='blue', lw=4,\n",
    "         label=f'Micro-Average ROC curve (AUC = {micro_auc_cv:.4f} ± {micro_auc_cv_std:.4f})')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('SVM Model Cross Validation Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Weighted-Average ROC Curve for cross-validation\n",
    "plt.plot(fpr_weighted_cv, tpr_weighted_cv, color='red', lw=2,\n",
    "         label=f'Weighted-Average ROC curve (AUC = {weighted_auc_cv:.4f} ± {weighted_auc_cv_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "micro_auc_best=0.9552\n",
    "micro_auc_best_std=0.0023\n",
    "\n",
    "\n",
    "weighted_auc_best=0.9411\n",
    "weighted_auc_best_std=0.0023\n",
    "\n",
    "# 绘制最佳参数模型的 AUC 和方差的 ROC 曲线\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Micro-Average ROC Curve for best model\n",
    "plt.plot(fpr_micro_best, tpr_micro_best, color='blue', lw=4,\n",
    "         label=f'Micro-Average ROC curve (AUC = {micro_auc_best:.4f} ± {micro_auc_best_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('SVM Best Model Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Weighted-Average ROC Curve for best model\n",
    "plt.plot(fpr_weighted_best, tpr_weighted_best, color='red', lw=2,\n",
    "         label=f'Weighted-Average ROC curve (AUC = {weighted_auc_best:.4f} ± {weighted_auc_best_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest model and train it\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on test sets\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "precision = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for the random forest model\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Method of selecting samples for training each tree\n",
    "}\n",
    "\n",
    "# Create a random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Setting Grid Search Parameters\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring='accuracy',  # Use accuracy as the evaluation metric\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_history = rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print the best parameters and best model score\n",
    "print(\"Best parameters found: \", rf_grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", rf_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of RF model hyperparametric combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = pd.DataFrame(rf_grid_search.cv_results_)\n",
    "\n",
    "rf_results_acc = rf_results[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "rf_results_acc\n",
    "\n",
    "rf_results_acc.to_excel('rf_grid_search_results.xlsx', index=False)\n",
    "\n",
    "print(\"Results saved to 'rf_grid_search_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# use the best model to make predictions\n",
    "cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Calculate the average accuracy\n",
    "mean_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the accuracy of each fold and the average accuracy\n",
    "print(f\"Per-fold accuracy for 10-fold cross validation: {cv_scores}\")\n",
    "print(f\"Average accuracy of 10-fold cross-validation: {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model to make predictions\n",
    "best_model_rf = rf_grid_search.best_estimator_\n",
    "y_pred_rf = best_model_rf.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "precision = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# print Performance Score\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3]\n",
    "y_train_bin = label_binarize(y_train, classes=classes)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "# 10-fold cross-validation\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "y_scores_cv = cross_val_predict(rf_model, X_train_scaled, y_train, cv=10, method=\"predict_proba\", n_jobs=-1)\n",
    "\n",
    "# compute Micro-Average AUC\n",
    "micro_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"micro\")\n",
    "micro_auc_cv_std = np.std(cross_val_score(rf_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "# compute Weighted-Average AUC\n",
    "weighted_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"weighted\")\n",
    "weighted_auc_cv_std = np.std(cross_val_score(rf_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "print(f\"10fold cross validation Micro-Average AUC: Mean = {micro_auc_cv:.4f}, Std Dev = {micro_auc_cv_std:.4f}\")\n",
    "print(f\"10fold cross validation Weighted-Average AUC: Mean = {weighted_auc_cv:.4f}, Std Dev = {weighted_auc_cv_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model \n",
    "best_model_rf= RandomForestClassifier(n_estimators=200, max_depth=30, min_samples_leaf=1, min_samples_split=2, bootstrap=False, random_state=42)\n",
    "\n",
    "best_model_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_scores_best = best_model_rf.predict_proba(X_test_scaled)\n",
    "\n",
    "# compute Micro-Average ROC curve\n",
    "micro_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"micro\")\n",
    "micro_auc_best_std = np.std(cross_val_score(best_model_rf, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "# compute Weighted-Average ROC curve\n",
    "weighted_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"weighted\")\n",
    "weighted_auc_best_std = np.std(cross_val_score(best_model_rf, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "print(f\"The best Micro-Average AUC: Mean = {micro_auc_best:.4f}, Std Dev = {micro_auc_best_std:.4f}\")\n",
    "print(f\"The best Weighted-Average AUC: Mean = {weighted_auc_best:.4f}, Std Dev = {weighted_auc_best_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "micro_auc_cv=0.9736\n",
    "micro_auc_cv_std=0.0010\n",
    "weighted_auc_cv=0.9685\n",
    "weighted_auc_cv_std=0.0010\n",
    "\n",
    "\n",
    "# 绘制交叉验证的 AUC 和方差的 ROC 曲线\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Micro-Average ROC Curve for cross-validation\n",
    "plt.plot(fpr_micro_cv, tpr_micro_cv, color='blue', lw=4,\n",
    "         label=f'Micro-Average ROC curve (AUC = {micro_auc_cv:.4f} ± {micro_auc_cv_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('RF Model Cross Validation Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Weighted-Average ROC Curve for cross-validation\n",
    "plt.plot(fpr_weighted_cv, tpr_weighted_cv, color='red', lw=2,\n",
    "         label=f'Weighted-Average ROC curve (AUC = {weighted_auc_cv:.4f} ± {weighted_auc_cv_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "micro_auc_best=0.9802\n",
    "micro_auc_best_std=0.0010\n",
    "\n",
    "weighted_auc_best=0.9756\n",
    "weighted_auc_best_std=0.0010\n",
    "\n",
    "\n",
    "# 绘制最佳参数模型的 AUC 和方差的 ROC 曲线\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Micro-Average ROC Curve for best model\n",
    "plt.plot(fpr_micro_best, tpr_micro_best, color='blue', lw=4,\n",
    "         label=f'Micro-Average ROC curve (AUC = {micro_auc_best:.4f} ± {micro_auc_best_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('RF Best Model Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Weighted-Average ROC Curve for best model\n",
    "plt.plot(fpr_weighted_best, tpr_weighted_best, color='red', lw=2,\n",
    "         label=f'Weighted-Average ROC curve (AUC = {weighted_auc_best:.4f} ± {weighted_auc_best_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost & LightGBM Model Part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBosot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a XGBoost model and train it\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on test sets\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "precision = precision_score(y_test, y_pred_xgb, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_xgb, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "\n",
    "# print Performance \n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV, KFold\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# create a XGBoost model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Define the parameter grid for the XGBoost model\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],    # Number of trees\n",
    "    'max_depth': [3, 5, 7],             # Maximum depth of the tree\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
    "    'subsample': [0.8, 1.0],            # Subsample ratio of the training instances\n",
    "    'colsample_bytree': [0.8, 1.0]      # Subsample ratio of columns when constructing each tree\n",
    "}\n",
    "\n",
    "# Create a 10-fold cross-validation object\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a Grid Search object\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print the best parameters and best model score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", grid_search.best_score_)\n",
    "\n",
    "# use the best model to make predictions\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "y_pred_xgb = best_xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "precision = precision_score(y_test, y_pred_xgb, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_xgb, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "\n",
    "# print Performance\n",
    "print(\"\\nModel Performance Metrics on Test Data:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of XGBoost model hyperparametric combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "xgboost_results_acc = xgboost_results[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "xgboost_results_acc\n",
    "\n",
    "xgboost_results_acc.to_excel('xbg_grid_search_results.xlsx', index=False)\n",
    "\n",
    "print(\"Results saved to 'xbg_grid_search_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(colsample_bytree=1.0,learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8, random_state=42)\n",
    "# use the best model to make predictions\n",
    "cv_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Calculate the average accuracy\n",
    "mean_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the accuracy of each fold and the average accuracy\n",
    "print(f\"Per-fold accuracy for 10-fold cross validation: {cv_scores}\")\n",
    "print(f\"Average accuracy of 10-fold cross-validation: {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3]\n",
    "y_train_bin = label_binarize(y_train, classes=classes)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "y_scores_cv = cross_val_predict(xgb_model, X_train_scaled, y_train, cv=10, method=\"predict_proba\", n_jobs=-1)\n",
    "\n",
    "# compute Micro-Average AUC\n",
    "micro_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"micro\")\n",
    "micro_auc_cv_std = np.std(cross_val_score(xgb_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "# 计算Weighted-Average AUC\n",
    "weighted_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"weighted\")\n",
    "weighted_auc_cv_std = np.std(cross_val_score(xgb_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "print(f\"10fold cross validation Micro-Average AUC: Mean = {micro_auc_cv:.4f}, Std Dev = {micro_auc_cv_std:.4f}\")\n",
    "print(f\"10fold cross validation Weighted-Average AUC: Mean = {weighted_auc_cv:.4f}, Std Dev = {weighted_auc_cv_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model = XGBClassifier(colsample_bytree=1.0,learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8, random_state=42)\n",
    "\n",
    "best_xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# compute the auc for the best model\n",
    "y_scores_best = best_xgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# compute Micro-Average ROC curve\n",
    "micro_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"micro\")\n",
    "micro_auc_best_std = np.std(cross_val_score(best_xgb_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "# compute Weighted-Average ROC curve\n",
    "weighted_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"weighted\")\n",
    "weighted_auc_best_std = np.std(cross_val_score(best_xgb_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "print(f\"The best Micro-Average AUC: Mean = {micro_auc_best:.4f}, Std Dev = {micro_auc_best_std:.4f}\")\n",
    "print(f\"The best Weighted-Average AUC: Mean = {weighted_auc_best:.4f}, Std Dev = {weighted_auc_best_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt the roc curve for 10-fold cross-validation\n",
    "fpr_micro_cv, tpr_micro_cv, _ = roc_curve(y_train_bin.ravel(), y_scores_cv.ravel())\n",
    "fpr_weighted_cv, tpr_weighted_cv, _ = roc_curve(y_train_bin.ravel(), y_scores_cv.ravel())\n",
    "\n",
    "micro_auc_cv=0.9782\n",
    "micro_auc_cv_std=0.0007\n",
    "weighted_auc_cv=0.9745\n",
    "weighted_auc_cv_std=0.0007\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.plot(fpr_micro_cv, tpr_micro_cv, color='blue', lw=4, label=f'Micro-Average ROC curve (AUC = {micro_auc_cv:.4f} ± {micro_auc_cv_std:.4f})')\n",
    "plt.plot(fpr_weighted_cv, tpr_weighted_cv, color='red', lw=2, label=f'Weighted-Average ROC curve (AUC = {weighted_auc_cv:.4f} ± {weighted_auc_cv_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('XGBoost Model Cross Validation Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt the roc curve for the best model\n",
    "fpr_micro_best, tpr_micro_best, _ = roc_curve(y_test_bin.ravel(), y_scores_best.ravel())\n",
    "fpr_weighted_best, tpr_weighted_best, _ = roc_curve(y_test_bin.ravel(), y_scores_best.ravel())\n",
    "\n",
    "micro_auc_best=0.9841\n",
    "micro_auc_best_std=0.0009\n",
    "\n",
    "weighted_auc_best=0.9803\n",
    "weighted_auc_best_std=0.0009\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.plot(fpr_micro_best, tpr_micro_best, color='blue', lw=4, label=f'Micro-Average ROC curve (AUC = {micro_auc_best:.4f} ± {micro_auc_best_std:.4f})')\n",
    "plt.plot(fpr_weighted_best, tpr_weighted_best, color='red', lw=2, label=f'Weighted-Average ROC curve (AUC = {weighted_auc_best:.4f} ± {weighted_auc_best_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('XGBoost Best Model Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], linestyle='--', color='grey', lw=2)\n",
    "\n",
    "# # 去掉网格\n",
    "# plt.grid(False)\n",
    "\n",
    "# # 设置图形的范围、标签、标题和图例\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.grid(False)\n",
    "# plt.xlabel('False Positive Rate', fontsize=15)\n",
    "# plt.ylabel('True Positive Rate', fontsize=15)\n",
    "# plt.title('LR Best Model Micro & Weighted-Average ROC', fontsize=18)\n",
    "# plt.legend(loc=\"lower right\", fontsize=12)\n",
    "\n",
    "# # 显示图形\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lightGBM_model\n",
    "lightGBM_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=len(y.unique()),\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#  fit model\n",
    "lightGBM_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on test sets\n",
    "y_pred_lightGBM = lightGBM_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_lightGBM)\n",
    "precision = precision_score(y_test, y_pred_lightGBM, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_lightGBM, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_lightGBM, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lightGBM))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lightGBM, digits=4)) # set the number of decimal places to 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for the LightGBM model\n",
    "lightGBM_param_grid = {\n",
    "    'boosting_type': ['gbdt', 'dart'],  # Gradient Boosting Decision Tree\n",
    "    'num_leaves': [31, 50, 100],        # Maximum number of leaves in one tree\n",
    "    'max_depth': [-1, 10, 20],          # Maximum depth of the tree\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],  # \n",
    "    'n_estimators': [100, 200, 300],    # Number of trees\n",
    "    'objective': ['multiclass'],        # multiclass\n",
    "    'num_class': [4],                   # Number of classes\n",
    "}\n",
    "\n",
    "# create a LightGBM model\n",
    "lightGBM_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Setting Grid Search Parameters\n",
    "lightGBM_grid_search = GridSearchCV(\n",
    "    estimator=lightGBM_model,\n",
    "    param_grid=lightGBM_param_grid,\n",
    "    cv=10,  # \n",
    "    scoring='accuracy',  # \n",
    "    n_jobs=-1,  # \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "lightGBM_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print the best parameters and best model score\n",
    "print(\"Best parameters found: \", lightGBM_grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", lightGBM_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model to make predictions\n",
    "best_lightGBM_model = lightGBM_grid_search.best_estimator_\n",
    "y_pred_lightGBM = best_lightGBM_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating Accuracy, Precision, Recall and F1 Scores\n",
    "accuracy = accuracy_score(y_test, y_pred_lightGBM)\n",
    "precision = precision_score(y_test, y_pred_lightGBM, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_lightGBM, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_lightGBM, average='weighted')\n",
    "\n",
    "# print Performance Score\n",
    "print(\"\\nModel Performance Metrics on Test Data:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print Confusion Matrix and Classification Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lightGBM))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lightGBM, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3]\n",
    "y_train_bin = label_binarize(y_train, classes=classes)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "lightGBM_model = LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=4,\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_scores_cv = cross_val_predict(lightGBM_model, X_train_scaled, y_train, cv=10, method=\"predict_proba\", n_jobs=-1)\n",
    "\n",
    "# compute Micro-Average AUC\n",
    "micro_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"micro\")\n",
    "micro_auc_cv_std = np.std(cross_val_score(xgb_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "# 计算Weighted-Average AUC\n",
    "weighted_auc_cv = roc_auc_score(y_train_bin, y_scores_cv, average=\"weighted\")\n",
    "weighted_auc_cv_std = np.std(cross_val_score(xgb_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "print(f\"10fold cross validation Micro-Average AUC: Mean = {micro_auc_cv:.4f}, Std Dev = {micro_auc_cv_std:.4f}\")\n",
    "print(f\"10fold cross validation Weighted-Average AUC: Mean = {weighted_auc_cv:.4f}, Std Dev = {weighted_auc_cv_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lightgbm_model = LGBMClassifier(objective='multiclass',\n",
    "    num_class=4,\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.2,\n",
    "    n_estimators=300,\n",
    "    max_depth= -1,\n",
    "    n_leaves=100,\n",
    "    random_state=42)\n",
    "\n",
    "best_lightgbm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# compute the auc for the best model\n",
    "y_scores_best = best_lightgbm_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# compute Micro-Average ROC curve\n",
    "micro_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"micro\")\n",
    "micro_auc_best_std = np.std(cross_val_score(best_lightgbm_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "# compute Weighted-Average ROC curve\n",
    "weighted_auc_best = roc_auc_score(y_test_bin, y_scores_best, average=\"weighted\")\n",
    "weighted_auc_best_std = np.std(cross_val_score(best_lightgbm_model, X_train_scaled, y_train, cv=10, scoring='roc_auc_ovr'))\n",
    "\n",
    "print(f\"The best Micro-Average AUC: Mean = {micro_auc_best:.4f}, Std Dev = {micro_auc_best_std:.4f}\")\n",
    "print(f\"The best Weighted-Average AUC: Mean = {weighted_auc_best:.4f}, Std Dev = {weighted_auc_best_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt the roc curve for 10-fold cross-validation\n",
    "fpr_micro_cv, tpr_micro_cv, _ = roc_curve(y_train_bin.ravel(), y_scores_cv.ravel())\n",
    "fpr_weighted_cv, tpr_weighted_cv, _ = roc_curve(y_train_bin.ravel(), y_scores_cv.ravel())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_micro_cv, tpr_micro_cv, color='blue', lw=4, label=f'Micro-Average ROC curve (AUC = {micro_auc_cv:.4f} ± {micro_auc_cv_std:.4f})')\n",
    "plt.plot(fpr_weighted_cv, tpr_weighted_cv, color='red', lw=2, label=f'Weighted-Average ROC curve (AUC = {weighted_auc_cv:.4f} ± {weighted_auc_cv_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('LightGBM Model Cross Validation Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# plt the roc curve for the best model\n",
    "fpr_micro_best, tpr_micro_best, _ = roc_curve(y_test_bin.ravel(), y_scores_best.ravel())\n",
    "fpr_weighted_best, tpr_weighted_best, _ = roc_curve(y_test_bin.ravel(), y_scores_best.ravel())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_micro_best, tpr_micro_best, color='blue', lw=4, label=f'Micro-Average ROC curve (AUC = {micro_auc_best:.4f} ± {micro_auc_best_std:.4f})')\n",
    "plt.plot(fpr_weighted_best, tpr_weighted_best, color='red', lw=2, label=f'Weighted-Average ROC curve (AUC = {weighted_auc_best:.4f} ± {weighted_auc_best_std:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('LightGBM Best Model Micro & Weighted-Average ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of LightGBM model hyperparametric combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightGBM_results = pd.DataFrame(lightGBM_grid_search.cv_results_)\n",
    "\n",
    "lightgbm_results_acc = lightGBM_results[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "lightgbm_results_acc\n",
    "\n",
    "lightgbm_results_acc.to_excel('lightgbm_grid_search_results.xlsx', index=False)\n",
    "\n",
    "print(\"Results saved to 'lightgbm_grid_search_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightGBM_model = LGBMClassifier(\n",
    "    boosting_type='gbdt', learning_rate=0.2, max_depth= -1, \n",
    "    n_estimators=300, num_class=4, num_leaves=100, objective='multiclass', random_state=42)\n",
    "\n",
    "# 'boosting_type': 'gbdt', 'learning_rate': 0.2, 'max_depth': -1, 'n_estimators': 300, 'num_class': 4, 'num_leaves': 100, 'objective': 'multiclass\n",
    "# use the best model to make predictions\n",
    "cv_scores = cross_val_score(lightGBM_model, X_train_scaled, y_train, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Calculate the average accuracy\n",
    "mean_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the accuracy of each fold and the average accuracy\n",
    "print(f\"Per-fold accuracy for 10-fold cross validation: {cv_scores}\")\n",
    "print(f\"Average accuracy of 10-fold cross-validation: {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variables\n",
    "X_cnn = df_numeric_imputed.drop('survival_category', axis=1).values\n",
    "y_cnn = df_numeric_imputed['survival_category'].values\n",
    "\n",
    "# standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled_cnn = scaler.fit_transform(X_cnn)\n",
    "\n",
    "# one-hot encode the target variable\n",
    "y_categorical_cnn = to_categorical(y_cnn)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_scaled_cnn, y_categorical_cnn, test_size=0.2, random_state=42)\n",
    "\n",
    "# check the shape of the training and testing sets\n",
    "print(f\"training num: {X_train_cnn.shape[0]}, features: {X_train_cnn.shape[1]}\")\n",
    "print(f\"testing num: {X_test_cnn.shape[0]}, features: {X_test_cnn.shape[1]}\")\n",
    "print(f\"training num: {y_train_cnn.shape[0]}, features: {y_train_cnn.shape[1]}\")\n",
    "print(f\"testing num: {y_test_cnn.shape[0]}, features: {y_test_cnn.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1_l2\n",
    "\n",
    "# 调整输入数据的形状以适应Conv1D\n",
    "X_train_cnn = X_train_cnn.reshape((X_train_cnn.shape[0], X_train_cnn.shape[1], 1))\n",
    "X_test_cnn = X_test_cnn.reshape((X_test_cnn.shape[0], X_test_cnn.shape[1], 1))\n",
    "\n",
    "\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# because of the data, using Conv1D\n",
    "cnn_model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))) # add regularization\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(y_categorical_cnn.shape[1], activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit model \n",
    "X_train_cnn = X_train_cnn.reshape((X_train_cnn.shape[0], X_train_cnn.shape[1], 1))  # 添加一个维度用于Conv1D\n",
    "X_test_cnn = X_test_cnn.reshape((X_test_cnn.shape[0], X_test_cnn.shape[1], 1))\n",
    "\n",
    "# set early_stopping\n",
    "cnn_early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=32, \n",
    "                          validation_data=(X_test_cnn, y_test_cnn), \n",
    "                          callbacks=[cnn_early_stopping], verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = cnn_model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "print(f\"the accuracy of testing set: {accuracy:.4f}\")\n",
    "\n",
    "# plot the training history\n",
    "y_pred_cnn = cnn_model.predict(X_test_cnn)\n",
    "y_pred_classes = np.argmax(y_pred_cnn, axis=1)\n",
    "y_true_classes = np.argmax(y_test_cnn, axis=1)\n",
    "\n",
    "# calculate the performance metrics\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "# print the performance metrics\n",
    "print(\"\\nModel Performance Metrics on Test Data:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nconfusion matirx:\")\n",
    "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
    "\n",
    "print(\"\\nclassification_report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(cnn_history.history['loss'], label='Training Loss')\n",
    "plt.plot(cnn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('CNN Model - Loss Fuction Trend')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the shape of the input data to fit Conv1D\n",
    "X_train_cnn = X_train_cnn.reshape((X_train_cnn.shape[0], X_train_cnn.shape[1], 1))\n",
    "X_test_cnn = X_test_cnn.reshape((X_test_cnn.shape[0], X_test_cnn.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_cnn_model(optimizer='adam', dropout_rate=0.5):\n",
    "    input_layer = Input(shape=(X_cnn.shape[1], 1))\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Wrapping Models with Scikeras' KerasClassifier\n",
    "cnn_classifier = KerasClassifier(model=create_cnn_model, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# define param_grid\n",
    "param_grid = {\n",
    "    'model__optimizer': ['adam', 'rmsprop'],\n",
    "    'model__dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [20, 30, 50]\n",
    "}\n",
    "\n",
    "# \n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "#  set Grid Search\n",
    "cnn_grid_search = GridSearchCV(estimator=cnn_classifier, param_grid=param_grid, cv=kfold, n_jobs=-1, verbose=2)\n",
    "\n",
    "# fit the model by Grid Search\n",
    "cnn_grid_search.fit(X_scaled_cnn.reshape((X_scaled_cnn.shape[0], X_scaled_cnn.shape[1], 1)), y_categorical_cnn, verbose=1)\n",
    "\n",
    "\n",
    "# print the best parameters and best model score\n",
    "print(\"Best parameters found: \", cnn_grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", cnn_grid_search.best_score_)\n",
    "\n",
    "# use the best model to make predictions\n",
    "best_cnn_model = cnn_grid_search.best_estimator_\n",
    "y_pred_cnn = best_cnn_model.predict(X_test_cnn)\n",
    "\n",
    "# compute the confusion matrix and classification report\n",
    "y_pred_classes = np.argmax(y_pred_cnn, axis=1)\n",
    "y_true_classes = np.argmax(y_test_cnn, axis=1)\n",
    "\n",
    "# calculate the performance metrics\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "# print Performance \n",
    "print(\"\\nModel Performance Metrics on Test Data:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n confusion_matrix:\")\n",
    "print(confusion_matrix(y_true_classes, y_pred_classes))\n",
    "\n",
    "print(\"\\n classification_report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, digits=4)) # set the number of decimal places to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_results = pd.DataFrame(cnn_grid_search.cv_results_)\n",
    "\n",
    "cnn_results_acc = cnn_results[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "cnn_results_acc\n",
    "\n",
    "cnn_results_acc.to_excel('cnn_grid_search_results.xlsx', index=False)\n",
    "\n",
    "print(\"Results saved to 'cnn_grid_search_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3]\n",
    "\n",
    "# Ensure that X_train_scaled is reshaped properly for CNN\n",
    "# X_train_scaled_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "\n",
    "# 训练模型\n",
    "X_train_cnn = X_train_cnn.reshape((X_train_cnn.shape[0], X_train_cnn.shape[1], 1))  # 添加一个维度用于Conv1D\n",
    "X_test_cnn = X_test_cnn.reshape((X_test_cnn.shape[0], X_test_cnn.shape[1], 1))\n",
    "\n",
    "def create_cnn_model(optimizer='adam', dropout_rate=0.5):\n",
    "    input_layer = Input(shape=(X_train_scaled.shape[1], 1))\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "cnn_early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# 确保输入数据的形状适合Conv1D\n",
    "# X_train_cnn = X_scaled_cnn.reshape((X_scaled_cnn.shape[0], X_scaled_cnn.shape[1], 1))\n",
    "\n",
    "# 定义CNN模型包装器\n",
    "cnn_classifier = KerasClassifier(model=create_cnn_model, batch_size=64, epochs=30, dropout_rate=0.5, \n",
    "                                 optimizer='adam', random_state=42, verbose=2)\n",
    "\n",
    "# 设置10折交叉验证\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 计算交叉验证的准确率\n",
    "cv_scores = cross_val_score(cnn_classifier, X_train_cnn, y_train_cnn, cv=kfold, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# 打印每折的准确率和平均准确率\n",
    "print(f\"Per-fold accuracy for 10-fold cross-validation: {cv_scores}\")\n",
    "print(f\"Average accuracy of 10-fold cross-validation: {cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "y_train_cnn = to_categorical(y_train, num_classes=4)  # 假设4个类别\n",
    "\n",
    "def create_cnn_model(optimizer='adam', dropout_rate=0.5):\n",
    "    input_layer = Input(shape=(X_train_cnn.shape[1], 1))  # 使用X_train_cnn的形状\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')#, kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(4, activation='softmax')(x)  # 假设有4个类别\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "# Binarize the labels for multi-class ROC AUC\n",
    "y_categorical_cnn = label_binarize(y_train_cnn, classes=[0, 1, 2, 3])\n",
    "\n",
    "# 10-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_micro_aucs = []\n",
    "cv_weighted_aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs_micro = []\n",
    "tprs_weighted = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_train_cnn, np.argmax(y_train_cnn, axis=1)):\n",
    "    X_train_fold, X_val_fold = X_train_cnn[train_idx], X_train_cnn[val_idx]\n",
    "    y_train_fold, y_val_fold = y_categorical_cnn[train_idx], y_categorical_cnn[val_idx]\n",
    "\n",
    "    # Reshape for Conv1D input\n",
    "    X_train_fold = X_train_fold.reshape((X_train_fold.shape[0], X_train_fold.shape[1], 1))\n",
    "    X_val_fold = X_val_fold.reshape((X_val_fold.shape[0], X_val_fold.shape[1], 1))\n",
    "\n",
    "    # Train the model\n",
    "    best_cnn_model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_pred_fold = best_cnn_model.predict(X_val_fold)\n",
    "\n",
    "    # Calculate AUC for each fold\n",
    "    micro_auc = roc_auc_score(y_val_fold, y_pred_fold, average='micro')\n",
    "    weighted_auc = roc_auc_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "\n",
    "    cv_micro_aucs.append(micro_auc)\n",
    "    cv_weighted_aucs.append(weighted_auc)\n",
    "\n",
    "    # Compute ROC curve and AUC for micro-average\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_val_fold.ravel(), y_pred_fold.ravel())\n",
    "    tprs_micro.append(np.interp(mean_fpr, fpr_micro, tpr_micro))\n",
    "    tprs_micro[-1][0] = 0.0\n",
    "\n",
    "# Calculate mean and std deviation of the AUCs\n",
    "cv_micro_auc_mean = np.mean(cv_micro_aucs)\n",
    "cv_micro_auc_std = np.std(cv_micro_aucs)\n",
    "cv_weighted_auc_mean = np.mean(cv_weighted_aucs)\n",
    "cv_weighted_auc_std = np.std(cv_weighted_aucs)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plotting the micro-average ROC curve\n",
    "mean_tpr_micro = np.mean(tprs_micro, axis=0)\n",
    "mean_tpr_micro[-1] = 1.0\n",
    "mean_auc_micro = auc(mean_fpr, mean_tpr_micro)\n",
    "plt.plot(mean_fpr, mean_tpr_micro, color='blue', linestyle='-', lw=5,\n",
    "         label=f'Micro-Average ROC (AUC = {cv_micro_auc_mean:.4f} ± {cv_micro_auc_std:.4f})')\n",
    "\n",
    "# Plotting the weighted-average ROC curve\n",
    "plt.plot(mean_fpr, mean_tpr_micro, color='red', linestyle='-', lw=2,\n",
    "         label=f'Weighted-Average ROC (AUC = {cv_weighted_auc_mean:.4f} ± {cv_weighted_auc_std:.4f})')\n",
    "\n",
    "# Diagonal line (random guessing)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10-Fold Cross-Validation ROC Curves for CNN Model')\n",
    "plt.legend(loc='lower right')\n",
    "# 去掉网格\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"10-Fold Cross-Validation Micro-Average AUC: Mean = {cv_micro_auc_mean:.4f}, Std Dev = {cv_micro_auc_std:.4f}\")\n",
    "print(f\"10-Fold Cross-Validation Weighted-Average AUC: Mean = {cv_weighted_auc_mean:.4f}, Std Dev = {cv_weighted_auc_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "# Binarize the test labels for AUC calculation\n",
    "y_test_binarized = label_binarize(y_test_cnn, classes=[0, 1, 2, 3])\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = best_cnn_model.predict_proba(X_test_cnn)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_pred_proba.ravel())\n",
    "roc_auc[\"micro\"] = roc_auc_score(y_test_binarized, y_pred_proba, average=\"micro\")\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i]) * np.sum(y_test_binarized[:, i])\n",
    "\n",
    "# Average it and compute AUC\n",
    "mean_tpr /= np.sum(y_test_binarized)\n",
    "roc_auc[\"weighted\"] = np.trapz(mean_tpr, all_fpr)\n",
    "\n",
    "# Compute the standard deviation of the AUC\n",
    "micro_auc_std = np.std([roc_auc[i] for i in range(n_classes)])\n",
    "weighted_auc_std = np.std([roc_auc[i] for i in range(n_classes)])\n",
    "\n",
    "# plt roc curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Micro-Average ROC curve\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'Micro-Average ROC (AUC = {roc_auc[\"micro\"]:.4f} ± {micro_auc_std:.4f})', color='blue', linestyle='-',lw=5)\n",
    "\n",
    "# Weighted-Average ROC curve\n",
    "plt.plot(all_fpr, mean_tpr, label=f'Weighted-Average ROC (AUC = {roc_auc[\"weighted\"]:.4f} ± {weighted_auc_std:.4f})', color='red', linestyle='-',lw=2)\n",
    "\n",
    "# Diagonal line (random guessing)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for CNN Model')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(False)  # drop the grid\n",
    "plt.show()\n",
    "\n",
    "# print the AUC values\n",
    "print(f\"Micro-Average AUC: {roc_auc['micro']:.4f} ± {micro_auc_std:.4f}\")\n",
    "print(f\"Weighted-Average AUC: {roc_auc['weighted']:.4f} ± {weighted_auc_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
